---
title             : "Item Characteristic Curves generated from common CTT Item Statistics"
shorttitle        : "CTT ICCs"
author: 
  - name          : "Diego Figueiras"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Dickson Hall 226"
    email         : "figueirasd1@montclair.edu"
  - name          : "John T. Kulas"
    affiliation   : "1"
    role:

affiliation:
  - id            : "1"
    institution   : "Montclair State University"


authornote: 

abstract: |
  Item characteristic curves (ICC's) are visual indicators of important attributes of assessment items - most typically *difficulty* and *discrimination*. Assessment specialists who examine ICC's usually do so from within the psychometric framework of either Item Response Theory (IRT) or Rasch modeling. We propose an extension of this tradition into the Classical Test Theory (CTT) framework. We first propose an application of ICCs with CTT indices then simulate data and quantify similarities and discrepancies between the IRT- and CTT-generated ICCs. Initial results are predominantly promising regaring ICC comparability. 
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib", "articles.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

csl               : "apa7.csl"
documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_word
---

```{r setup, include = FALSE}
library("papaja")
library(psych)
library(reticulate)
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed, echo=FALSE, warning=FALSE, message=FALSE)
```

Item characteristic curves are frequently referenced by psychometricians as visual indicators of important attributes of assessment items - most frequently *difficulty* and *discrimination*. Within these visual presentations the x-axis ranges along "trait" levels (by convention annotated with the greek $\theta$), whereas the y-axis displays probabilities of responding to the item within a given response category. In the context of true tests, the response categories are binary, and the y-axis probability refers to the likelihood of a "correct" response. From this visualization, the observer extracts the likelihood that respondents of any trait level would answer a focal item correctly. If the curve transitions from low to high likelihood at a location toward the lower end of the trait (e.g., "left" on the plotting surface), this indicates that it is relatively easy to answer the item correctly. Stated differently, it does not take much $\theta$ to have a high likelihood of answering correctly. On the contrary, if the growth in the curve occurs primarily at higher trait levels, this indicates that the item is relatively more difficult. If the curve is sharp (e.g., strongly vertical), this indicates high discrimination; if it is flatter, that is an indication of poorer discrimination.  

```{r, include=TRUE, fig.cap="Item characteristic curves primarily reflecting differences in discrimination.", echo=FALSE, warning=FALSE, message=FALSE, fig.height=4}

data<-read.csv("simulated_data.csv", header=FALSE)
#data$v30<-abs(data$v30-1)
library(mirt)
library(latticeExtra)
pseudob<-abs(qnorm(.5))

ahat<-function(x){
  r<-(((2.71828)^x)-(2.71828)^-x)/(2.71828-(2.71828)^x)
  ((0.51+(0.02*pseudob)+(0.301*pseudob^2))*r)
  
}
pseudoa<-ahat(.3)
c <- 0
#change pseudob in this line for a scale that allows negative numbers
p <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa*(x-pseudob))))))}

curve(p, from=-5, to=5, ylim=c(0, 1), main="Item Characteristic Curves", xlab="Level of Trait")
pseudob<-abs(qnorm(.2))
pseudoa<-ahat(.7)
curve(p, lty="longdash", add=TRUE)
arrows(-3,0.4,-1.5,0.4,col="black")
text(-3.8,0.4, "Low \n Discrimination",col="black")
arrows(2,0.2,0.45,0.2,col="black")
text(3,0.2, "High \n Discrimination",col="black")

```

Assessment specialists who examine ICC's usually do so from within the psychometric framework of either Item Response Theory (IRT) or Rasch modeling. These frameworks provide the parameters necessary to plot the ogive functions. Rasch models only estimate difficulty, and assume that differences in discrimination represent flaws in measurement. The IRT 2 parameter logistic model (2PL), however, models both item difficulty as well as item discrimination. Item difficulty (the *b*-parameter) is scaled as the trait level associated with a 50% likelihood of correct response (e.g., it is scaled to $\theta$). Item discrimination (*a*-parameter) is the degree to which an item differentiates across individuals who are characterized as being relatively lower or higher on the trait.  From a Classical Test Theory (CTT) orientation, item  difficulty is most commonly represented by the percent of individuals answering the item correctly (also referred to as a *p-value*). Item discrimination can be conveyed via a few different CTT indices, but the most commonly calculated and consulted index is the corrected item total correlation. 

Assessment specialists who consult these CTT parameters don’t typically (to our knowledge!) attempt to represent them visually, as is common in IRT and Rasch applications. However, there is perhaps little opposition for them not to do so, as ICC's based on CTT parameters could provide snapshot psychometric information as valuable as those gained from IRT- or Rasch-derived ICC's. The largest obstacle to psychometricians deeming CTT-derived visuals to be of value is likely tied to the concept of invariance, which refers to IRT parameter independence across item and person estimates. However, this property is often overstated, as invariance is only attained with perfect model-data fit (which never occurs), and is also only true after being subjected to linear transformation (commonly across samples). Additionally, several comparative investigations have noted commonality between IRT and CTT difficulty and discrimination estimates as well as relative stability of CTT estimates when samples are large and/or judisciously constructed [@fan1998item]. Fan in fact summarizes that the IRT and CTT frameworks “...produce very similar item and person statistics” (p.379). @hambleton1993comparison concluded that "no study provides enough empirical evidence on the extent of disparity between the two frameworks and the superiority of IRT over CTT despite the theoretical differences". 

## CTT and IRT comparability investigations

@fan1998item looked at the correlations between ability estimates and item difficulty in CTT and all three IRT models. These correlations were very high, generally between .80 and .90, showing a lot of overlap between the two methodologies. As for item discrimination, correlations were moderate to high, with only a few being very low.[^footie] 

[^footie]:...and in fact, as is presented below, the relationship between the IRT and CTT discrimination indices is non-linear - the correlation is an inappropriate index to capture the magnitude of this relationship.

@fan1998item also investigated item invariance for all models. In theory, the major advantage of IRT models over CTT is that the latter has an interdependency between the item and person statistics, whereas IRT has no such dependency. For example, within CTT examinations, the average item difficulty is equivalent to the average person score - these indices are merely reflective of averages computed across rows or columns. What @fan1998item found in his study, however, did not support the purported invariant advantage of IRT parameters over CTT indices. Both CTT item difficulty and discrimination degrees of invariance were highly correlated with those of IRT, indicating that they were highly comparable. 

There has also been occasions in which the invariance parameter has been conceptualized as a graded continuum instead of a categorical population property [@hambleton1991fundamentals;@rupp2004note]. Estimates of IRT parameters across different calibration runs can be looked at for evidence of a possible lack of invariance. This doesn't happen with CTT item parameters, since they will always be sample-dependent. This dependency, however, is greatly influenced by the sampling strategy. Large scale data, truly random sampling, and large range items could give comparable CTT item and person statistics across testing populations and occasions [@kulas2017approximate]. Additionally, there are several empirical investigations that note high levels of "invariance" of CTT estimates, in some cases surpassing IRT item estimates in their capacity to have cross-sample stability [@macdonald2002monte; @fan1998item]. 

An adjustment to @lord2012applications's formula giving the functional relationship between the "non-invariant" CTT and "invariant" IRT statistics becomes useful in comparing the two methodologies, despite the supposed lack of invariance from CTT. So even though here we acknowledge that invariance is a categorical IRT property, we still follow the functional modification proposed by @kulas2017approximate, noting that having a large sample that is truly random and whose items are normally distributed and have a center at the moderate difficulty can help reduce threats to CTT "invariance". 

## Relationship between IRT and CTT indices

@lord2012applications described a function that approximates the relationship between the IRT *a*-parameter and the CTT discrimination index of an item-test biserial correlation:

$$a_i\cong \frac{r_i}{\sqrt{1-r_i^2}}$$

This formula wasn't intended for practical purposes but rather was specified in an attempt to help assessment specialists who were more familiar with CTT procedures to better understand the relationship to the IRT discrimination parameter. In an effort to move from the conceptual to a practical application, @kulas2017approximate proposed a modification that minimized the average residual (either $a_i$ or $r_i$, where $r_i$ is the *corrected* item-total *point-biserial* correlation). 

The @kulas2017approximate investigations (both simulated and utilizing real-world test data) identified systematic predictive differences across items with differing item difficulty values, so their recommended formula included a specification for item difficulty (this formulaic specification is also retained in the current presentation):

$$\hat{a_i}\cong[(.51 + .02z_g + .3z_g^2)r]+[(.57 - .009z_g + .19z_g^2)\frac{e^r-e^{-r}}{e-e^r}]$$

Where $g$ is the absolute deviation from 50% responding an item correctly and 50% responding incorrectly (e.g., a "p-value" of .5). $Z_g$ is the standard normal deviate associated with $g$. This transformation of the standard p-value was recommended in order to scale this index along an interval-level metric more directly analogous to the IRT *b*-parameter. Figure 2 visualizes the re-specifications of Lord's formula at p-values (difficulty) of .5, .3 (or .7), and .1 (or .9) and highlights the nonlinear nature of this relationship - especially noticeable at high(er) levels of discrimination.

```{r acorrected, fig.cap="Empricially-derived functional relationship between the IRT *a* parameter and the CTT corrected-item total correlation as a function of item difficulty (p-value; solid = .5, dashed = .3/.7, dotted = .1/.9)."}
g<-abs(qnorm(.5))
g<-0
r2<-.3
ahat<-function(r2){
  r<-(((2.71828)^r2)-(2.71828)^-r2)/(2.71828-(2.71828)^r2)
  ((0.51+(0.02*g)+(0.301*g^2))*r)
 
}

curve(ahat, from=0, to=1, ylim=c(0, 8), xname="Corrected Item-Total Correlation", ylab="IRT a-parameter")
g<-abs(qnorm(.7))
r2<-.7
curve(ahat, lty="longdash",add=TRUE )
g<-abs(qnorm(.1))
r2<-.1
curve(ahat, lty="dotted", add=TRUE)

```

As we can see, the higher the corrected item-total correlations, the higher the estimated IRT a-parameter (discrimination). Also, as the p-values (difficulty) deviates from 0, the relationship between the estimated IRT a-parameter and the corrected item-total correlations becomes stronger. 

Practitioners and researchers that don't use IRT or Rasch models and instead opt to follow a CTT philosophy would benefit from having ICC's that use CTT statistics. This study intends to show evidence of the overlapping nature of CTT and IRT parameters when it comes to plotting ICCs. 


# Study 1A - EsTABLISHING RELATIONSHIP BETWEEN THE irt AND ctt difficulty indices. 

## Method
iN ORDER TO ESTIMATE THE RELATIONSHIP, WE SIMULATED ITEMS. We used 50 items but then we generated 4 different distributions of p-values. One is that they're all uniform and roughly .5 values. Second is that they're all different but uniform (.1, .2, .3.... .99). Third is that they're normally distributed around a value of .5. Fourth is that they're a inverted normal distribution centered about .5. Fifth is that it is negatively skewed. Sixth is that it is positively skewed. 

```{r simulation}
library(descr)
# sim1<- matrix(rnorm(500000, mean=.5, sd=1), nrow=10000, ncol=50)
# 
# sim2<- data.frame(
#   v1=(rnorm(10000, 0.01, sd=1)),
#   v2=(rnorm(10000, 0.03, sd=1)),
#   v3=(rnorm(10000, 0.05, sd=1)),
#   v4=(rnorm(10000, 0.07, sd=1)),
#   v5=(rnorm(10000, 0.09, sd=1)),
#   v6=(rnorm(10000, 0.11, sd=1)),
#   v7=(rnorm(10000, 0.13, sd=1)),
#   v8=(rnorm(10000, 0.15, sd=1)),
#   v9=(rnorm(10000, 0.17, sd=1)),
#   v10=(rnorm(10000, 0.19, sd=1)),
#   v11=(rnorm(10000, 0.21, sd=1)),
#   v12=(rnorm(10000, 0.23, sd=1)),
#   v13=(rnorm(10000, 0.25, sd=1)),
#   v14=(rnorm(10000, 0.27, sd=1)),
#   v15=(rnorm(10000, 0.29, sd=1)),
#   v16=(rnorm(10000, 0.31, sd=1)),
#   v17=(rnorm(10000, 0.33, sd=1)),
#   v18=(rnorm(10000, 0.35, sd=1)),
#   v19=(rnorm(10000, 0.37, sd=1)),
#   v20=(rnorm(10000, 0.39, sd=1)),
#   v21=(rnorm(10000, 0.41, sd=1)),
#   v22=(rnorm(10000, 0.43, sd=1)),
#   v23=(rnorm(10000, 0.45, sd=1)),
#   v24=(rnorm(10000, 0.47, sd=1)),
#   v25=(rnorm(10000, 0.49, sd=1)),
#   v26=(rnorm(10000, 0.51, sd=1)),
#   v27=(rnorm(10000, 0.53, sd=1)),
#   v28=(rnorm(10000, 0.55, sd=1)),
#   v29=(rnorm(10000, 0.57, sd=1)),
#   v30=(rnorm(10000, 0.59, sd=1)),
#   v31=(rnorm(10000, 0.61, sd=1)),
#   v32=(rnorm(10000, 0.63, sd=1)),
#   v33=(rnorm(10000, 0.65, sd=1)),
#   v34=(rnorm(10000, 0.67, sd=1)),
#   v35=(rnorm(10000, 0.69, sd=1)),
#   v36=(rnorm(10000, 0.71, sd=1)),
#   v37=(rnorm(10000, 0.73, sd=1)),
#   v38=(rnorm(10000, 0.75, sd=1)),
#   v39=(rnorm(10000, 0.77, sd=1)),
#   v40=(rnorm(10000, 0.79, sd=1)),
#   v41=(rnorm(10000, 0.81, sd=1)),
#   v42=(rnorm(10000, 0.83, sd=1)),
#   v43=(rnorm(10000, 0.85, sd=1)),
#   v44=(rnorm(10000, 0.87, sd=1)),
#   v45=(rnorm(10000, 0.89, sd=1)),
#   v46=(rnorm(10000, 0.91, sd=1)),
#   v47=(rnorm(10000, 0.93, sd=1)),
#   v48=(rnorm(10000, 0.95, sd=1)),
#   v49=(rnorm(10000, 0.97, sd=1)),
#   v50=(rnorm(10000, 0.99, sd=1))
#   
#   
# )
# 
# sim3<-
# 
# sim4<-data.frame(
#   v1=(rnorm(10000, 0.1, sd=1)),
#   v2=(rnorm(10000, 0.1, sd=1)),
#   v3=(rnorm(10000, 0.1, sd=1)),
#   v4=(rnorm(10000, 0.1, sd=1)),
#   v5=(rnorm(10000, 0.1, sd=1)),
#   v6=(rnorm(10000, 0.1, sd=1)),
#   v7=(rnorm(10000, 0.1, sd=1)),
#   v8=(rnorm(10000, 0.1, sd=1)),
#   v9=(rnorm(10000, 0.1, sd=1)),
#   v10=(rnorm(10000, 0.2, sd=1)),
#   v11=(rnorm(10000, 0.2, sd=1)),
#   v12=(rnorm(10000, 0.2, sd=1)),
#   v13=(rnorm(10000, 0.2, sd=1)),
#   v14=(rnorm(10000, 0.2, sd=1)),
#   v15=(rnorm(10000, 0.2, sd=1)),
#   v16=(rnorm(10000, 0.2, sd=1)),
#   v17=(rnorm(10000, 0.2, sd=1)),
#   v18=(rnorm(10000, 0.2, sd=1)),
#   v19=(rnorm(10000, 0.2, sd=1)),
#   v20=(rnorm(10000, 0.2, sd=1)),
#   v21=(rnorm(10000, 0.2, sd=1)),
#   v22=(rnorm(10000, 0.2, sd=1)),
#   v23=(rnorm(10000, 0.2, sd=1)),
#   v24=(rnorm(10000, 0.2, sd=1)),
#   v25=(rnorm(10000, 0.2, sd=1)),
#   v26=(rnorm(10000, 0.8, sd=1)),
#   v27=(rnorm(10000, 0.8, sd=1)),
#   v28=(rnorm(10000, 0.8, sd=1)),
#   v29=(rnorm(10000, 0.8, sd=1)),
#   v30=(rnorm(10000, 0.8, sd=1)),
#   v31=(rnorm(10000, 0.8, sd=1)),
#   v32=(rnorm(10000, 0.8, sd=1)),
#   v33=(rnorm(10000, 0.8, sd=1)),
#   v34=(rnorm(10000, 0.8, sd=1)),
#   v35=(rnorm(10000, 0.9, sd=1)),
#   v36=(rnorm(10000, 0.9, sd=1)),
#   v37=(rnorm(10000, 0.9, sd=1)),
#   v38=(rnorm(10000, 0.9, sd=1)),
#   v39=(rnorm(10000, 0.9, sd=1)),
#   v40=(rnorm(10000, 0.9, sd=1)),
#   v41=(rnorm(10000, 0.9, sd=1)),
#   v42=(rnorm(10000, 0.9, sd=1)),
#   v43=(rnorm(10000, 0.9, sd=1)),
#   v44=(rnorm(10000, 0.9, sd=1)),
#   v45=(rnorm(10000, 0.9, sd=1)),
#   v46=(rnorm(10000, 0.9, sd=1)),
#   v47=(rnorm(10000, 0.9, sd=1)),
#   v48=(rnorm(10000, 0.9, sd=1)),
#   v49=(rnorm(10000, 0.97, sd=1)),
#   v50=(rnorm(10000, 0.99, sd=1))
#   
#   
# )
# 
# 
# sim5<- data.frame(
#   v1=(rnorm(10000, 0.01, sd=1)),
#   v2=(rnorm(10000, 0.03, sd=1)),
#   v3=(rnorm(10000, 0.05, sd=1)),
#   v4=(rnorm(10000, 0.07, sd=1)),
#   v5=(rnorm(10000, 0.09, sd=1)),
#   v6=(rnorm(10000, 0.11, sd=1)),
#   v7=(rnorm(10000, 0.13, sd=1)),
#   v8=(rnorm(10000, 0.15, sd=1)),
#   v9=(rnorm(10000, 0.17, sd=1)),
#   v10=(rnorm(10000, 0.19, sd=1)),
#   v11=(rnorm(10000, 0.21, sd=1)),
#   v12=(rnorm(10000, 0.23, sd=1)),
#   v13=(rnorm(10000, 0.25, sd=1)),
#   v14=(rnorm(10000, 0.27, sd=1)),
#   v15=(rnorm(10000, 0.29, sd=1)),
#   v16=(rnorm(10000, 0.31, sd=1)),
#   v17=(rnorm(10000, 0.33, sd=1)),
#   v18=(rnorm(10000, 0.35, sd=1)),
#   v19=(rnorm(10000, 0.37, sd=1)),
#   v20=(rnorm(10000, 0.39, sd=1)),
#   v21=(rnorm(10000, 0.41, sd=1)),
#   v22=(rnorm(10000, 0.43, sd=1)),
#   v23=(rnorm(10000, 0.45, sd=1)),
#   v24=(rnorm(10000, 0.47, sd=1)),
#   v25=(rnorm(10000, 0.49, sd=1)),
#   v26=(rnorm(10000, 0.51, sd=1)),
#   v27=(rnorm(10000, 0.53, sd=1)),
#   v28=(rnorm(10000, 0.55, sd=1)),
#   v29=(rnorm(10000, 0.6, sd=1)),
#   v30=(rnorm(10000, 0.6, sd=1)),
#   v31=(rnorm(10000, 0.7, sd=1)),
#   v32=(rnorm(10000, 0.7, sd=1)),
#   v33=(rnorm(10000, 0.8, sd=1)),
#   v34=(rnorm(10000, 0.8, sd=1)),
#   v35=(rnorm(10000, 0.91, sd=1)),
#   v36=(rnorm(10000, 0.91, sd=1)),
#   v37=(rnorm(10000, 0.91, sd=1)),
#   v38=(rnorm(10000, 0.91, sd=1)),
#   v39=(rnorm(10000, 0.91, sd=1)),
#   v40=(rnorm(10000, 0.91, sd=1)),
#   v41=(rnorm(10000, 0.91, sd=1)),
#   v42=(rnorm(10000, 0.91, sd=1)),
#   v43=(rnorm(10000, 0.91, sd=1)),
#   v44=(rnorm(10000, 0.91, sd=1)),
#   v45=(rnorm(10000, 0.91, sd=1)),
#   v46=(rnorm(10000, 0.91, sd=1)),
#   v47=(rnorm(10000, 0.93, sd=1)),
#   v48=(rnorm(10000, 0.95, sd=1)),
#   v49=(rnorm(10000, 0.97, sd=1)),
#   v50=(rnorm(10000, 0.99, sd=1))
#   
#   
# )
# 
# sim6<- data.frame(
#   v1=(rnorm(10000, 0.01, sd=1)),
#   v2=(rnorm(10000, 0.03, sd=1)),
#   v3=(rnorm(10000, 0.05, sd=1)),
#   v4=(rnorm(10000, 0.07, sd=1)),
#   v5=(rnorm(10000, 0.09, sd=1)),
#   v6=(rnorm(10000, 0.09, sd=1)),
#   v7=(rnorm(10000, 0.09, sd=1)),
#   v8=(rnorm(10000, 0.09, sd=1)),
#   v9=(rnorm(10000, 0.09, sd=1)),
#   v10=(rnorm(10000, 0.09, sd=1)),
#   v11=(rnorm(10000, 0.09, sd=1)),
#   v12=(rnorm(10000, 0.09, sd=1)),
#   v13=(rnorm(10000, 0.09, sd=1)),
#   v14=(rnorm(10000, 0.09, sd=1)),
#   v15=(rnorm(10000, 0.09, sd=1)),
#   v16=(rnorm(10000, 0.09, sd=1)),
#   v17=(rnorm(10000, 0.09, sd=1)),
#   v18=(rnorm(10000, 0.09, sd=1)),
#   v19=(rnorm(10000, 0.09, sd=1)),
#   v20=(rnorm(10000, 0.09, sd=1)),
#   v21=(rnorm(10000, 0.09, sd=1)),
#   v22=(rnorm(10000, 0.2, sd=1)),
#   v23=(rnorm(10000, 0.2, sd=1)),
#   v24=(rnorm(10000, 0.3, sd=1)),
#   v25=(rnorm(10000, 0.3, sd=1)),
#   v26=(rnorm(10000, 0.3, sd=1)),
#   v27=(rnorm(10000, 0.4, sd=1)),
#   v28=(rnorm(10000, 0.4, sd=1)),
#   v29=(rnorm(10000, 0.57, sd=1)),
#   v30=(rnorm(10000, 0.59, sd=1)),
#   v31=(rnorm(10000, 0.61, sd=1)),
#   v32=(rnorm(10000, 0.63, sd=1)),
#   v33=(rnorm(10000, 0.65, sd=1)),
#   v34=(rnorm(10000, 0.67, sd=1)),
#   v35=(rnorm(10000, 0.69, sd=1)),
#   v36=(rnorm(10000, 0.71, sd=1)),
#   v37=(rnorm(10000, 0.73, sd=1)),
#   v38=(rnorm(10000, 0.75, sd=1)),
#   v39=(rnorm(10000, 0.77, sd=1)),
#   v40=(rnorm(10000, 0.79, sd=1)),
#   v41=(rnorm(10000, 0.81, sd=1)),
#   v42=(rnorm(10000, 0.83, sd=1)),
#   v43=(rnorm(10000, 0.85, sd=1)),
#   v44=(rnorm(10000, 0.87, sd=1)),
#   v45=(rnorm(10000, 0.89, sd=1)),
#   v46=(rnorm(10000, 0.91, sd=1)),
#   v47=(rnorm(10000, 0.93, sd=1)),
#   v48=(rnorm(10000, 0.95, sd=1)),
#   v49=(rnorm(10000, 0.97, sd=1)),
#   v50=(rnorm(10000, 0.99, sd=1))
#   
#   
# )
# sim1means<- data.frame(colMeans(sim1))  
# sim2means<-data.frame(colMeans(sim2)) 
# sim4means<-data.frame(colMeans(sim4))
# sim5means<-data.frame(colMeans(sim5))
# sim6means<-data.frame(colMeans(sim6))
# 
# par(mfrow=c(2,3))
# hist(sim1means$colMeans.sim1., xlim=c(0,1))
# hist(sim2means$colMeans.sim2.)
# hist(sim4means$colMeans.sim4.)
# hist(sim5means$colMeans.sim5.)
# hist(sim6means$colMeans.sim6.)



###################### SIMULATION 1: ALL 0.5 ##############################
x<-data.frame(matrix(runif(10000, 0, 1), ncol=100, nrow=10000))
sim1<-data.frame(ifelse(x<0.5, 0, 1))
sim1means<-colMeans(sim1)


###################### SIMULATION 2: UNIFORM DISTRIBUTION ##############################

x<-data.frame(matrix(runif(10000, 0, 1), ncol=100, nrow=10000))
sim2<-matrix(ncol=100, nrow=10000)
for(i in 1:100){
  sim2[,i]<-ifelse(x[,i]<0.01*i,0,1)
}

sim2means<-colMeans(sim2)


###################### SIMULATION 3: NORMAL DISTRIBUTION ##############################
y<-data.frame(matrix(rnorm(100,0,1)))
x<-data.frame(matrix(rnorm(10000,0,1), ncol=100, nrow=10000))
sim3<-matrix(ncol=100, nrow=10000)
for(i in 1:100){
  sim3[,i]<-ifelse(x[,i]<y[i,],0,1)
}
sim3means<-colMeans(sim3)

###################### SIMULATION 4: INVERTED DISTRIBUTION ##############################

x<-data.frame(matrix(rbeta(10000, 5, 2), ncol=50, nrow=10000))
sim4.1<-matrix(ncol=50, nrow=10000)
for(i in 1:ncol(x)){
  
  sim4.1[,i]<-ifelse(x[,i]<(0.02*i), 0,1)
}
sim4.2<-matrix(ncol=50, nrow=10000)
for(i in 1:ncol(x)){
  
  sim4.2[,i]<-ifelse(x[,i]>(0.02*i), 0,1)
}
sim4<-cbind(sim4.1, sim4.2)
sim4means<-colMeans(sim4)


###################### SIMULATION 5: SKEWED NEGATIVE ##############################


x<-data.frame(matrix((rbeta(10000,5,2)), ncol=100, nrow=10000))
sim5<-matrix(ncol=100, nrow=10000)
for(i in 1:ncol(x)){
  
  sim5[,i]<-ifelse(x[,i]<(0.01*i), 0,1)
}
sim5means<-colMeans(sim5)


###################### SIMULATION 6: SKEWED POSITIVE ##############################

x<-data.frame(matrix((rbeta(10000,5,2)), ncol=100, nrow=10000))
sim6<-matrix(ncol=100, nrow=10000)
for(i in 1:ncol(x)){
  
  sim6[,i]<-ifelse(x[,i]>(0.01*i), 0,1)
}
sim6means<-colMeans(sim6)


```





# Study 1B - Estimating CTT-derived ICC's

The purpose of study 1 is to look at the visualizations resulting from @kulas2017approximate formula on simulated data. We hypothesize that the relationship between the estimated IRT a-parameter and the corrected item-total correlations will be an exponential function instead of a linear relationship, and it will become stronger as the corrected item-total correlations and the p-values deviate from 0, which would mean that the item has more discrimination. 

## Procedure and methods

We simulated data using @han2007wingen3 software. Our sample was 10,000 observations, with a mean of 0 and a standard deviation of 1. The number of items were 100, with response categories of either correct or incorrect (1 and 0). The mean for the a-parameter for the simulated data was 2, and the standard deviation 0.8. The mean for the b-parameter was 0 and the standard deviation 0.5. The mirt package from @R-mirt was used to compute the IRT a-parameters and to plot the 2PL resulting model. As for the CTT-derived a-parameter, the modification to @lord2012applications's formula described earlier was used, as well as the re-scaling for the p-values. We additionally changed the scale of the difficulty estimates of CTT so they were on the same scale as the IRT estimates. This was done by building a regression model using the CTT a-estimate to predict the IRT a-parameter. The resulting values from this model were used in plotting the CTT-derived ICC's.  

## Results
As shown by Figure 2, our plot looks very similar to that of [@kulas2017approximate, p.8]. This confirms that our formula for computing the estimated a-parameter follows the exponential relationship we can see in [@lord2012applications; @kulas2017approximate]. Four random items were selected and plotted in Figure 3 using IRT and CTT-derived statistics. The blue curves were plotted using a IRT 2PL model, while the red curves were plotted with CTT-derived parameters. 

```{r plotting, results="hide", fig.cap="Four ICCs showcasing the difference between CTT and IRT-derivated curves at different levels." }

data<-read.csv("simulated_data.csv", header=FALSE)
#data$v30<-abs(data$v30-1)
library(mirt)
library(latticeExtra)
library(irtplay)
ahat<-function(x){
  r<-(((2.71828)^x)-(1/(2.71828)^x))/(2.71828-(2.71828)^x)
  
  ((0.51+(0.02*pseudob)+(0.301*pseudob^2))*r)+((0.57-(0.009*pseudob)+(0.19*pseudob^2))*r)
  
}

mod<-mirt(data, 1, itemtype="2PL")
# plot(mod, type="trace")
# 
alphas<-alpha(data)
citcs<-data.frame(alphas$item.stats$r.drop)
pseudoA<-data.frame(ahat(citcs))
pseudoB<-data.frame(qnorm(colMeans(data)))
IRT_parms <- coef(mod, IRTpars = TRUE, simplify = TRUE)
irt <- IRT_parms$items
df<-as.data.frame(cbind(citcs, pseudoA, pseudoB, irt))
colnames(df)<-c("CITC", "PseudoA", "PseudoB", "a", "b", "c1", "c2")

# plot(df$PseudoA, df$a)
# plot(df$b, df$PseudoB)

lm.reg<-lm(b ~PseudoB, data=df)
 

b<-0.01479-(-1.33142*pseudoB) 
dat<-data.frame(b, alphas$item.stats$r.drop)
colnames(dat)<-c("b", "corrected item totals")
par(cex.axis=1, cex.lab=1, cex.main=2, cex.sub=0.1)

###############################################################
pseudob<-dat$b[47]
ahat<-function(x){
  r<-(((2.71828)^x)-(1/(2.71828)^x))/(2.71828-(2.71828)^x)
  
  ((0.51+(0.02*pseudob)+(0.301*pseudob^2))*r)+((0.57-(0.009*pseudob)+(0.19*pseudob^2))*r)
  
}

pseudoa<-ahat(dat$`corrected item totals`[47])
c <- 0

eq <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa*(x-pseudob))))))}

p1<-plot(mod, which.items=c(47), main=FALSE, sub="Moderate DIF \n(area between curves = 0.36 )", cex.sub=0.2)+latticeExtra::layer(panel.curve(eq, col="red"))
################################################################

pseudob2<-dat$b[1]
ahat<-function(x){
  r<-(((2.71828)^x)-(1/(2.71828)^x))/(2.71828-(2.71828)^x)
  
  ((0.51+(0.02*pseudob2)+(0.301*pseudob2^2))*r)+((0.57-(0.009*pseudob2)+(0.19*pseudob2^2))*r)
  
}

pseudoa2<-ahat(dat$`corrected item totals`[1])
c <- 0

eq2 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa2*(x-pseudob2))))))}

p2<-plot(mod, which.items=c(1),main=FALSE, sub="Small DIF \n(area between curves = 0.03)", cex.sub=0.2)+latticeExtra::layer(panel.curve(eq2, col="red"))


#####################################################################

pseudob3<-dat$b[54]
ahat<-function(x){
  r<-(((2.71828)^x)-(1/(2.71828)^x))/(2.71828-(2.71828)^x)
  
  ((0.51+(0.02*pseudob3)+(0.301*pseudob3^2))*r)+((0.57-(0.009*pseudob3)+(0.19*pseudob3^2))*r)
  
}

pseudoa3<-ahat(dat$`corrected item totals`[54])
c <- 0

eq3 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa3*(x-pseudob3))))))}

p3<-plot(mod, which.items=c(54), main=FALSE, sub="Small DIF \n(area between curves  = 0.09)", cex.sub=0.2)+latticeExtra::layer(panel.curve(eq3, col="red"))

###############################################################################
pseudob4<- dat$b[25]
ahat<-function(x){
  r<-(((2.71828)^x)-(1/(2.71828)^x))/(2.71828-(2.71828)^x)
  
  ((0.51+(0.02*pseudob4)+(0.301*pseudob4^2))*r)+((0.57-(0.009*pseudob4)+(0.19*pseudob4^2))*r)
  
}

pseudoa4<-ahat(dat$`corrected item totals`[25])
c <- 0

eq4 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa4*(x-pseudob4))))))}

p4<-plot(mod, which.items=c(25), main=FALSE, sub="Large DIF \n(area between curves = 0.81)", cex.main=5)+latticeExtra::layer(panel.curve(eq4, col="red", cex.sub=1))

###############################################################################
require(gridExtra)
grid.arrange(p2,p1,p3,p4, top="Item Characteristic Curves",nrow=2, ncol=2)

##############################################################################



```


# Study 2 - Evaluating the Comparability of IRT and CTT ICC's 

The purpose of study 2 is to simulate test data and generate ICC's based on the IRT model. Then we compare that to our CTT estimates and look at the differences. We hypothesize that on average there won't be a big difference between the curves plotted with either methodology. 

## Procedure and materials  

The same simulated data as in study 1 was used. The mirt package from @R-mirt was used to compute and plot the IRT statistics. As we can see on Figure 3, the blue curves were plotted using 2PL IRT parameters (a and b), while the red curves were plotted using CTT parameters (p-values and corrected item-total correlations, re-scaling and modifying them with @kulas2017approximate formulas). To quantify the degree of difference between the two curves, the Area Between Curves was computed using @R-geiger_a's package. This procedure was done for all 100 items. 


## Results

We used `r cite_r("r-references.bib")` for all our analyses.  -->

The area between ICC's was calculated between CTT-derived and IRT-derived ICC's. The average difference for all 100 curves was 0.35. As we can see in Figure 4, most of the data is skewed towards the lower end, indicating that out of the 100 items, most of them have areas between the curves of less than 0.35. 
For Figure 5 we plotted all the 100 ICC's that use CTT parameters, and for Figure 6 we did the same but with IRT parameters instead. Curves using both methodologies are very similar in shape and form, as we can see in the two items that we point out in each figure. 



```{r histrogram, results="hide", fig.cap="Histogram of all areas between curves plotted using IRT parameters vs curves plotted using CTT parameters."}
#Area between curves
#Preparing data
library(geiger)
citcs<-data.frame(alphas$item.stats$r.drop)
pseudoA<-data.frame(ahat(citcs))
pseudoB<-b
IRT_parms <- coef(mod, IRTpars = TRUE, simplify = TRUE)
irt <- IRT_parms$items
df<-as.data.frame(cbind(citcs, pseudoA, pseudoB, irt))
colnames(df)<-c("CITC", "PseudoA", "PseudoB", "a", "b", "c1", "c2")

#calculating AUC
theta <- matrix(seq(-6,6, by=.1))
# eq4 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[25]*(x-df$PseudoB[25]))))))}
# cttB<-eq4(seq(-6,6, by=.1))
# eq4_irt<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[25]*(x-df$b[25]))))))}
# irtB<-eq4_irt(seq(-6,6, by=.1))
# geiger:::.area.between.curves(theta, cttB, irtB)
# x is the vector of x-axis values
# f1 the y-axis values for the first line
# f2 the y-axis values for the second line

#Looping
auc<-rep(NA, nrow(df))

for (i in 1:nrow(df)){
  eq_CTT<- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[i]*(x-df$PseudoB[i]))))))}
  cttB<-eq_CTT(seq(-6,6, by=.1))
  eq_IRT<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[i]*(x-df$b[i]))))))}
  irtB<-eq_IRT(seq(-6,6, by=.1))
  auc[i]<-abs(geiger:::.area.between.curves(theta, cttB, irtB))
}

h<-hist(auc, col="red", xlab="Areas Between Curves",
   main="Histogram of Areas Between ICCs")
h
xfit<-seq(min(auc),max(auc),length=40)
yfit<-dnorm(xfit,mean=mean(auc),sd=sd(auc))
yfit <- yfit*diff(h$mids[1:2])*length(auc)
lines(xfit, yfit, col="blue", lwd=2)





```


```{r cttcurves, results="hide",fig.cap="ICCs derived from only CTT parameters (with two noteworthy ICCs annotated)."}
p_ctt<-0
p_irt<-0
colors<-rep(c("Red", "blue","yellow","orange","purple","brown","green","pink","black", "white"), 10)
eq_CTT<- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[1]*(x-df$PseudoB[1]))))))}
p_ctt<-curve(eq_CTT, xlim=c(-4,4), xlab="Level of Trait", ylab="p(x)")
for (i in 1:nrow(df)){
  eq_CTT<- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[i]*(x-df$PseudoB[i]))))))}
  p_ctt[i]<-curve(eq_CTT, col=colors[i], xlim=c(-4,4), add=TRUE)
  
  cttB<-eq_CTT(seq(-6,6, by=.1))
  eq_IRT<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[i]*(x-df$b[i]))))))}
  irtB<-eq_IRT(seq(-6,6, by=.1))
  auc[i]<-abs(geiger:::.area.between.curves(theta, cttB, irtB))
}
p_ctt
arrows(3.2,0.85,2.2,0.85,col="purple")
text(3.7,0.85, "Item 4",col="purple")
arrows(2,0.4,1.5,0.4,col="black")
text(2.7,0.4, "Item 24",col="black")

```


```{r irtcurves, results="hide",fig.cap="Typical ICCs derived from IRT parameters (same noteworthy items annotated)."}
eq_IRT<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[1]*(x-df$b[1]))))))}
p_irt<-curve(eq_IRT, xlim=c(-4,4), xlab="Level of Trait", ylab="p(x)")

for (i in 1:nrow(df)){
    eq_IRT<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[i]*(x-df$b[i]))))))}
    p_irt[i]<-curve(eq_IRT, col=colors[i], xlim=c(-4,4), add=TRUE, xlab="Level of Trait", ylab="p(x)")
}

p_irt
arrows(3,0.8,2,0.8,col="purple")
text(3.7,0.8, "Item 4",col="purple")
arrows(2,0.4,1.5,0.4,col="black")
text(2.7,0.4, "Item 24",col="black")

```



# Discussion

Important psychometric information can be gathered from ICC's, which are visual indicators typically of difficulty and discrimination. Psychometricians and other assessment specialists usually examine ICC's under the lenses of IRT and Rasch models. From a CTT orientation, item difficulty is most commonly represented by the percent of individuals answering the item correctly (also referred to as a p-value). Item discrimination can be conveyed via a few CTT indices, but the most commonly calculated and consulted index is the corrected item-total correlation. Assessment specialists who consult these CTT parameters don’t typically attempt to represent them visually, as is common in IRT and Rasch applications. However, there is perhaps little reason for them not to do so, as ICC's based on CTT parameters could provide snapshot psychometric information as valuable as those gained from IRT- or Rasch-derived ICC's. Here we first propose an application of ICC's with CTT indices, then we simulated data and quantified similarities and discrepancies between the IRT- and CTT-generated ICC's. Our hypothesis was that the Area Between Curves of these different ICC's would be small. Area between curves for 100 items was 0.35 on average. This result indicates that curves plotted with either IRT or CTT parameters show little difference. The nature of both models is mostly overlapping when it comes to plotting visual representations such as ICC's. Practitioners and researchers that don’t use IRT or Rasch models and instead opt to follow a CTT philosophy would benefit from having ICC's that use CTT statistics.

If this general idea is well-received (SIOP members would seem to represent a great barometer!) we would like to stress the CTT ICC's via further and more extensive conditions. That is, are there patterns that help explain CTT ICCs that diverge from their IRT counterparts? Although our simulations did generate a range of item difficulties and discriminations, we have not yet fully explored systematic patterns of extremely difficult/easy items as well as very poorly discriminating items. If patterns emerge, we would like to model predicted discrepancies via incorporating error bars within our visualizations. 

represent some promise regarding plotted ICC's using IRT and CTT parameters. Our hypothesis was that the Area Between Curves of these different ICCs would be small. Area between curves for 100 items was 0.35 on average. This result indicates that curves plotted with either IRT or CTT parameters show little difference. The nature of both models is overlapping when it comes to plotting visual representations such as ICC's. Practitioners and researchers that don't use IRT or Rasch models and instead opt to follow a CTT philosophy would benefit from having ICC's that use CTT statistics.

Additionally, if there is interest in this general idea we would likely publish our function as a small `R` package, perhaps to supplement the `psych` package's "alpha" function, which produces corrected item-total correlations as well as p-values within the same output table (e.g., the "input" data is already available in tabular format).

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
