---
title             : "Item Characteristic Curve estimation from common Classical Test Theory indices"
shorttitle        : "CTT ICCs"
author: 
  - name          : "Diego Figueiras"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Dickson Hall 226"
    email         : "figueirasd1@montclair.edu"
  - name          : "John T. Kulas"
    affiliation   : "2"

affiliation:
  - id            : "1"
    institution   : "Montclair State University"
  - id            : "2"
    institution   : "eRg"

authornote: 

abstract: |
  Item characteristic curves (ICC's) are graphical representations of important attributes of assessment items - most commonly *difficulty* and *discrimination*. Assessment specialists who examine ICC's usually do so from within the psychometric framework of either Item Response Theory (IRT) or Rasch modeling. We propose an extension of this tradition of item characteristic visualization within the more commonly leveraged Classical Test Theory (CTT) framework. We first simulate binary (e.g., true *test*) data with varying item difficulty characteristics to generate empirically-derived linking coefficients between the IRT and CTT difficulty indices. The results of these simulations provided some degree of confidence regarding functional linking invariance. Next, we simulated datasets of varying item characteristic specification and generated ICCs derived from both IRT and CTT frameworks. Differential item functioning (DIF) was estimated by calculating the geometric area between the IRT- and CTT-derived ogives. The average DIF estimate was .2. Applying the CTT-derived ICCs to an applied sample of XXX test takers resulted in a mean DIF estimate of XXX. An `R` package, `ctticc`, performs the ICC calculations presented in the current paper and provides assessment specialists with visual representations of CTT-derived item characteristics.
  
keywords          : "Classical Test Theory, Item Response Theory, item difficulty, item discrimination"
wordcount         : "X"

bibliography      : ["r-references.bib", "articles.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

csl               : "apa7.csl"
documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library(psych)
library(reticulate)
r_refs("r-references.bib", append=FALSE)         ## append=FALSE auto-updates package version 
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed, echo=FALSE, warning=FALSE, message=FALSE)
```

```{r example, include=TRUE, fig.cap="Item characteristic curves demonstrating differences in item difficulty and discrimination.", echo=FALSE, warning=FALSE, message=FALSE, out.width = "100%", out.height="80%"}


data<-read.csv("simulated_data.csv", header=FALSE)
#data$v30<-abs(data$v30-1)
library(mirt)
library(latticeExtra)
pseudob<-abs(qnorm(.5))

ahat<-function(x){
  r<-(((2.71828)^x)-(2.71828)^-x)/(2.71828-(2.71828)^x)
  ((0.51+(0.02*pseudob)+(0.301*pseudob^2))*r)
  
}
pseudoa<-ahat(.3)
c <- 0
#change pseudob in this line for a scale that allows negative numbers

pseudob  <-       qnorm(.02)   ## note these "p-values" operate in reverse
pseudoa  <-        ahat(.3)

pseudob2  <-  abs(qnorm(.5))
pseudoa2  <-       ahat(.7)

pseudob3  <-      qnorm(.99)
pseudoa3  <-       ahat(.7)

pseudob4  <-  abs(qnorm(.5))
pseudoa4  <-       ahat(.1)

colors<-c("Moderate Discrimination & Low Difficulty"="#aeb0af", 
          "High Discrimination & Moderate Difficulty"="#7c807d", 
          "High Discrimination & High Difficulty"="#1c1c1c", 
          "Low Discrimination & Moderate Difficulty"="#dfe0e0")

linetype<-c("Moderate Discrimination & Low Difficulty"="solid", 
          "High Discrimination & Moderate Difficulty"="dashed", 
          "High Discrimination & High Difficulty"="longdash", 
          "Low Discrimination & Moderate Difficulty"="dotted")

p <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa*(x-pseudob))))))}
p2 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa2*(x-pseudob2))))))}
p3 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa3*(x-pseudob3))))))}
p4<- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa4*(x-pseudob4))))))}

library(ggplot2)
library(scales)
base <-
  ggplot()


base+
  xlim(-3,3)+
  geom_function(fun=p, size=1.5, linetype="dotted", aes(color="Moderate Discrimination & Low Difficulty"))+
  geom_function(fun=p2, size=2, linetype="solid", aes(color="High Discrimination & Moderate Difficulty"))+
  geom_function(fun=p3, size=2, linetype="twodash", aes(color="High Discrimination & High Difficulty"))+
  geom_function(fun=p4, size=1.5, linetype="dotdash", aes(color="Low Discrimination & Moderate Difficulty"))+
  labs(x="Theta",
       y="p(1.0)",
       color="Legend")+
  scale_color_manual(values=colors)+
#  scale_linetype_manual(values = linetype)+         # didn't work (10/13/22)
  theme(legend.position="bottom")+
  guides(colour = guide_legend(nrow = 2))

```

Item characteristic curves are frequently consulted by psychometricians as visual indicators of important attributes of assessment items - most commonly *difficulty* and *discrimination*. Within these visual presentations the x-axis ranges along "trait" levels (by convention typically denoted with the greek $\theta$), whereas the y-axis displays probabilities of responding to the item within a given response category. In the context of true tests, the response categories are binary[^bock], and the y-axis probability reflects the likelihood of a "correct" response[^binary]. Assessment specialists who consult ICC's usually do so from within the psychometric framework of either Item Response Theory (IRT) or Rasch modeling. These frameworks estimate the parameters necessary to plot the visual functions. Rasch models only estimate difficulty, and assume that differences in discrimination represent flaws in measurement. The IRT 2 parameter logistic model (2PL), however, estimates item discrimination in addition to item difficulty. 

When interpreting an ICC, the observer extracts the relationship between a respondent's trait level and the expectation of answering the item correctly. If the curve transitions from low to high likelihood at a location toward the lower end of the trait (e.g., "left" on the plotting surface), this indicates that it is relatively easy to answer the item correctly. Stated in the parlance of IRT or Rasch traditions, it does not take much $\theta$ to have a high likelihood of answering correctly. On the contrary, if the growth in the curve occurs primarily at higher trait levels, this indicates that the item is relatively more difficult. Through the lens of IRT, if discrimination is modeled and the curve is sharp (e.g., strongly vertical), this indicates high discrimination; if it is flatter, that is an indication of poorer discrimination (see Figure \@ref(fig:example)).  

[^bock]: With exception [see, for example, @masters1982rasch; @muraki1997generalized].

[^binary]: Because the historical convention in test response is to code a correct response as "1" and an incorrect response as "0", the y-axis is commonly denoted as "*p*(1)" or "*p*(1.0)".

Item difficulty (the *b*-parameter) is scaled to the trait level associated with a 50% likelihood of correct response (e.g., it is scaled to $\theta$). Item discrimination (the *a*-parameter) is the degree to which an item differentiates across individuals who are characterized as being relatively lower or higher on the trait and is scaled to the slope of the ICC function at the same 50% likelihood of correct response location[^another].  From a Classical Test Theory (CTT) orientation, item  difficulty is most commonly represented by the percent of individuals answering the item correctly (also referred to as a *p-value*). Item discrimination can be conveyed via a few different CTT indices, but the most commonly calculated and consulted index is the corrected item-total correlation. 

[^another]: Within the 2PL. If more item characteristics are modeled, the *a*-parameter may be estimated at a different function location. $\leftarrow$ Diego check this (look into *a*-paramter scaling for the 3PL; should be halfway between lower and upper asymptotes but I'm not 100% sure).

Assessment specialists who calculate these CTT item indices don’t typically (to our limited knowledge!) attempt to represent them visually, as is common in IRT and Rasch applications. However, ICC's based on CTT indices could possibly provide snapshot psychometric information as valuable as those gained from IRT- or Rasch-derived item parameters. The largest obstacle to psychometricians deeming CTT-derived visuals to be of value is likely tied to the concept of invariance, which refers to IRT parameter independence across item and person estimates. However, this property is often overstated, as invariance is only attained with perfect model-data fit (which never occurs), and is also only true after being subjected to linear transformation - commonly across samples [@rupp2006understanding]. Additionally, several comparative investigations have noted commonality between IRT and CTT difficulty and discrimination estimates as well as relative stability of CTT estimates when samples are large and/or judisciously constructed [@fan1998item]. Fan in fact summarizes that the IRT and CTT frameworks “...produce very similar item and person statistics” (p.379). @hambleton1993comparison state that "no study provides enough empirical evidence on the extent of disparity between the two frameworks and the superiority of IRT over CTT despite the theoretical differences". 

## CTT and IRT Comparability Investigations

@fan1998item examined correlations between CTT item statistics and the parameters derived from the three most popular *true test* IRT models (the 1-, 2-, and 3-parameter logistic). These correlations were very high, generally between .80 and .90. As for item discrimination, correlations were moderate to high, with only a few being very low[^footie]. @fan1998item also investigated index invariance for all models. In theory, the major advantage of IRT models over CTT is that the latter has an interdependency between the item and person statistics, whereas under ideal circumstances IRT parameters have no such dependency. For example, within CTT examinations, the average item difficulty is equivalent to the average person score - these indices are merely reflective of averages computed across rows or columns. What @fan1998item reported in his study, however, did not support the purported invariant advantage of IRT parameters over CTT indices. Both CTT-derived item difficulty and discrimination indices exhibited similar levels of invariance to the IRT-derived parameters, indicating that they were highly comparable. 

[^footie]: And in fact, as is presented below, the relationship between the IRT and CTT discrimination indices is non-linear - the Pearson's product moment correlation is therefore *not* the most appropriate index to capture the nature of this relationship.

## Functional Relationship(s) between IRT and CTT Indices

@lord1980applications described a function to approximates the nonlinear relationship between the IRT *a*-parameter and the CTT discrimination index[^Lord]:

[^Lord]: @lord1980applications's CTT discrimination index is actually the item-test biserial correlation as opposed to the contemporarily more popular *corrected* item-total *point-biserial* correlation. 

\begin{equation}
a_i\cong \frac{r_i}{\sqrt{1-r_i^2}}
\end{equation}

This formula wasn't intended for practical purposes but rather was specified in an attempt to help assessment specialists who were more familiar with CTT procedures to better understand the relationship to the IRT discrimination parameter. In an effort to move from the conceptual to a practical application, @kulas2017approximate proposed a modification that minimized the average residual (either $a_i$ or $r_i$, where $r_i$ is the *corrected* item-total *point-biserial* correlation). 

The @kulas2017approximate investigations identified systematicly predictive differences in the relationship between $a_i$ and $r_i$ across items with differing item difficulty values, so their alteration to @lord1980applications's formula included a qualifier effect for item difficulty (this formulaic specification is also retained in the current presentation):

\begin{equation}
\hat{a_i}\cong[(.51 + .02z_g + .3z_g^2)r]+[(.57 - .009z_g + .19z_g^2)\frac{e^r-e^{-r}}{e-e^r}]
\end{equation}

Where $g$ is the absolute deviation from 50% responding an item correctly and 50% responding incorrectly (e.g., a "p-value" of .5). $z_g$ is the standard normal deviate associated with $g$. This transformation of the common p-value was recommended by @kulas2017approximate in order to scale the CTT index along a (closer to) interval-level metric more directly analogous to the IRT *b*-parameter. Figure \@ref(fig:acorrected) visualizes the re-specifications of Lord's formula at p-values (difficulty) of .5, .3 (or .7), and .1 (or .9) and highlights the nonlinear nature of this relationship - especially noticeable at high(er) levels of discrimination.

```{r acorrected, fig.cap="@kulas2017approximate functional relationship between the IRT *a* parameter and the CTT corrected-item total correlation as a function of item difficulty (p-value; solid = .5, dashed = .3/.7, dotted = .1/.9)."}
g<-abs(qnorm(.5))
g<-0
r2<-.5          ## changed from .3 - 10/13/22
ahat<-function(r2){
  r<-(((2.71828)^r2)-(2.71828)^-r2)/(2.71828-(2.71828)^r2)
  ((0.51+(0.02*g)+(0.301*g^2))*r)
 
}

curve(ahat, from=0, to=1, ylim=c(0, 8), xname="Corrected Item-Total Correlation", ylab="IRT a-parameter")
g<-abs(qnorm(.7))
r2<-.7
curve(ahat, lty="longdash",add=TRUE )
g<-abs(qnorm(.1))
r2<-.1
curve(ahat, lty="dotted", add=TRUE)

```

# Study 1 

@kulas2017approximate provided an extension of @lord1980applications's conceptual formula, facilitating the scaling of the CTT corrected item-total correlation to the metric of the IRT *a*-parameter. @fan1998item demonstrated strong associations between the CTT p-value and IRT *b*-parameter, but did not attempt a scaling linkage. The ultimate goal of the current study is to generate CTT-derived ICCs. As a comparative standard, we also endeavor to compare the CTT-derived ICCs against IRT-derived ICCs. This comparison is only possible if the CTT statistic can be expressed on the IRT parameter metric (or vice versa). Study 1 therefore endeavors to develop a linking equation such that the CTT p-value may be translated to an IRT *b*-parameter metric.  Specifically, we establish a regression equation predicting a "psuedo" b parameter from the $z_{g}$ estimate ($z_{g}$ is a derivative of the CTT p-value). The reason we're doing this is to get a better x-axis value for the ICCs (better means closer to the IRT-derived *b*).

Although the ogives could be specified directly from the CTT-derived statistics, we made a procedural decision to retain the IRT 2PL as our function specification: 

\begin{equation}
P(\Theta)=\frac{1}{1+e^{-1.7a(\Theta-b)}}
\end{equation}

Our procedure therefore required the estimation of "pseudo" IRT parameters from the CTT indices. The *a* parameter was estimated via the formula specified in  @kulas2017approximate, while the *b* parameter was estimated via linking parameters identified via simulation.

1. Purpose: Getting a p-value $\rightarrow$ b-parameter linking equation            [X]
2. Five different distributions of p-values (simulated data)                        [ ]
3. 10,000 runs each simulation (100 items, 10,000 "people" each)                    [ ]  
4. Scrub extreme values (p-values essentially 0 and 1 need to be deleted)           [ ]
5. Moderated regression to look for differences across p-value distribution         [ ]
6. Tada!                                                                            [ ]  

## Method

```{r simulatedgraphs, results="hide", fig.cap="Shape of prescribed distributions of p-values across Study 1 conditions."}

dparabola <- function(x){ifelse(x < 1 | x > 5, 0, (3/16)*(x-3)^2)}
library(ggplot2)
g1<-ggplot()+xlim(1,5)+geom_function(fun=dparabola, size=1.5)+theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank(),
      plot.title = element_text(size=10))+
  xlab("Condition 3")+
  ggtitle("Inverted Normal")
g2<-ggplot()+xlim(-3,3)+geom_function(fun=dnorm, size=1.5)+theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank(),
      plot.title = element_text(size=10))+
  xlab("Condition 2")+
  ggtitle("Normal")
g3<-ggplot()+xlim(0,1)+geom_density(aes(rbeta(1000000, 5, 2)), size=1.5)+theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank(),
      plot.title = element_text(size=10))+
  xlab("Condition 4")+
  ggtitle("Negatively Skewed")
g4<-ggplot()+xlim(0,1)+geom_density(aes(rbeta(1000000, 2, 5)),size=1.5)+theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank(),
      plot.title = element_text(size=10))+
  xlab("Condition 5")+
  ggtitle("Positively Skewed")
g5<-ggplot()+xlim(-0.1,1.1)+geom_function(fun=dunif, size=1.5)+theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank(), 
      plot.title = element_text(size=10))+
  xlab("Condition 1")+
  ggtitle("Uniform")

require(gridExtra)
grid.arrange(g5,g2,g1,g3,g4, nrow=2, ncol=3)

```

In order to generate a linking equation between CTT and IRT indices, we simulated datasets primarily differing in item difficulty. We kept the item set equal (100 items per simulation) and specified 5 different distributions of item difficulties. The first distribution was uniform, with p-values ranging from low to high at roughly equal levels of frequency. The second distribution was effectively normal with p-values, centered around 0.5. The third distribution was an inverted normal distribution also centered around 0.5. The fourth distribution was a negatively skewed distribution of p-values, and the fifth was positively skewed. Figure \@ref(fig:simulatedgraphs) provides a visual representation of the distributional forms that were desired in our simulations. 

Then we computed regressions predicting the b-parameters using the standard normal deviate associated with the p-values on each simulation. The resulting regression coefficients for all simulations was approximately 2 and 0, indicating that our scaling was not sample dependent. 

## Procedure and methods

>*Note*. Maybe do a different linking via machine learning. Try to find the linking parameters (including p-value distributional shape and location) that minimize DIF across CTT and IRT ICCs (5/27/22 after unsuccessful Friday brainstorming especially regarding simulation 3 [the normally distributed p-values])

We built six simulations of IRT data using @Nydick, each with different shapes and p-values, as can be seen in figure 3. Our goal was to see if CTT statistics and IRT parameters would produce similar ICCs when simulated data of varying distributions are used. Each simulation consisted of 10,000 observations and 100 items. Simulation 1 was uniform, with p-values ranging from 0 to 1. Simultion 2 was a normal distribution with p-values centered around 0.5. Simulation 3 was an inverted U-shaped distribution, with p-values ranging from 0 to 1. Simulation 4 was a left skewed distribution with p-values centered around 0.5, and simulation 5 was a right skewed distribution with p-values centered around 0.5.All simulated distributions had an average a-estimate of 1.42 and average b-estimate of 0.5. We ran each simulation 1,000 times, each with 100 items and 10,000 cases. 

```{r}

## Need to grab new simulations (post SIOP) from Bixter's lab computer - 10/13/22

coeficients<-read.csv("pvalue_to_b_estimates_original.csv")
library(ggplot2)
ggplot(data = coeficients, aes(x = intercept)) + geom_histogram(bins = 500) + facet_grid(simulation~.)
ggplot(data = coeficients, aes(x = slope)) + geom_histogram(bins = 500) + facet_grid(simulation~.)


```


```{r, eval=FALSE}
library(tidyverse)
all_sims2<-read.csv("Simulation 2/all_sims2.csv")
all_sims3<-read.csv("Simulation 3/all_sims3.csv")
all_sims4<-read.csv("Simulation 4/all_sims4.csv")
all_sims5<-read.csv("Simulation 5/all_sims5.csv")
all_sims6<-read.csv("Simulation 6/all_sims6.csv")

all_sims2$condition<-'Simulation 2'
all_sims3$condition<-'Simulation 3'
all_sims4$condition<-'Simulation 4'
all_sims5$condition<-'Simulation 5'
all_sims6$condition<-'Simulation 6'
all_sims<-rbind(all_sims2, all_sims3, all_sims4, all_sims5, all_sims6)
all_sims<-all_sims%>%filter(b<3)%>%filter(b>-3)

reg<-lm(b~pseudob*condition, data=all_sims)
summary(reg)

ggplot(all_sims, aes(pseudob, b, color = factor(condition))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) 

sds<-coeficients%>%group_by(simulation)%>%summarise(sd_intercept=sd(intercept),
                                                    means_intercept=mean(intercept),
                                                    sd_slope=sd(slope),
                                                    means_slope=mean(slope))
reg2<-lm(b~pseudob, data=all_sims)
summary(reg2)
slope<-reg2$coefficients[[2]]
intercept<-reg2$coefficients[[1]]

```

```{r sendtoETS}
### Cross-validating with wingen simulation 
# 12/8/2022 our p-value to b-parameter simulations converged
# our CTTICC formula is now ready to share with others
# This chunk presents the information necessary to estimate CTT-ICCs and also evaluate
#their similarity to IRT generated ICCs

data<-read.csv("simulated_data.csv", header=FALSE) #reading wingen simulation

pseudob<-qnorm(colMeans(data))#calculating our Zg

c=0 #since we're using the 3PL specification, we set c to 0

library(mirt)
library(psych)
library(latticeExtra)

ahat<-function(x){
  r<-(((2.71828)^x)-(1/(2.71828)^x))/(2.71828-(2.71828)^x)
  
  ((0.51+(0.02*pseudob)+(0.301*pseudob^2))*r)+((0.57-(0.009*pseudob)+(0.19*pseudob^2))*r)
  
}#Formula taken from Kulas' 2017

mod<-mirt(data, 1, itemtype="2PL") #estimating the IRT-parameter from the wingen simulated data

alphas<-psych::alpha(data)#using the psych package to run alpha

citcs<-data.frame(alphas$item.stats$r.drop)#getting the corrected-item total correlations

pseudoA<-data.frame(ahat(citcs))#Applying Kula's 2017 formula to our corrected-item totals

## Getting all the parameters into one dataframe
IRT_parms <- coef(mod, IRTpars = TRUE, simplify = TRUE)#retrieving the IRT parameters from the mod object

irt <- IRT_parms$items

df<-as.data.frame(cbind(citcs, pseudoA, pseudob, irt))

colnames(df)<-c("CITC", "PseudoA", "PseudoB", "a", "b", "c1", "c2")

df$PseudoB<-0.000006957584+(-1.52731*df$PseudoB)#Using the regression coefficients computed on the simulations that converged on December 8 to modify our PseudoB

## Lines 352-363 create curves using our parameters and calculate the area between curves plotted with CTT and IRT parameters
theta <- matrix(seq(-6,6, by=.1))
auc<-rep(NA, nrow(df))

for (i in 1:nrow(df)){
  eq_CTT<- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[i]*(x-df$PseudoB[i]))))))}
  cttB<-eq_CTT(seq(-6,6, by=.1))
  eq_IRT<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[i]*(x-df$b[i]))))))}
  irtB<-eq_IRT(seq(-6,6, by=.1))
  f1 <- approxfun(theta, cttB-irtB)
  f2 <- function(x) abs(f1(x))          
  auc[i]<-integrate(f2, -6,6)
}
auc<-unlist(auc)
hist(auc)

## Plotting item 35, which has the worst DIF
pseudob<-df$b[35]
ahat<-function(x){
  r<-(((2.71828)^x)-(1/(2.71828)^x))/(2.71828-(2.71828)^x)
  
  ((0.51+(0.02*pseudob)+(0.301*pseudob^2))*r)+((0.57-(0.009*pseudob)+(0.19*pseudob^2))*r)
  
}

eq <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[35]*(x-df$PseudoB[35]))))))}
p1<-plot(mod, which.items=c(35), main=FALSE, sub="Moderate DIF \n(area between curves = 0.36 )", cex.sub=0.2, theta_lim = c(-4,4))+latticeExtra::layer(panel.curve(eq, col="red"))
p1


```


## Results
Overall, the average slope was *r slope* and the average intercept *r intercept*.
#put in moderated regression results as well and r-square change. Make sure this formula is in study 2

$$
\hat{b}\cong 0.000006957584-(1.52731\cdot{Zg})

$$


As shown by Figure 2, our plot looks very similar to that of [@kulas2017approximate, p.8]. This confirms that our formula for computing the estimated a-parameter follows the exponential relationship we can see in [@lord2012applications; @kulas2017approximate]. Four random items were selected and plotted in Figure 3 using IRT and CTT-derived statistics. The blue curves were plotted using a IRT 2PL model, while the red curves were plotted with CTT-derived parameters. 

# Study 2 - Evaluating the Comparability of IRT and CTT ICC's 

The purpose of study 2 is to simulate test data and generate ICC's based on the IRT model. Then we compare that to our CTT estimates and look at the differences. We hypothesize that on average there won't be a big difference between the curves plotted with either methodology. 

1. Use regression equation from Study 1
2. Many simulations 
3. Compute DIF and report results
4. IF POSSIBLE, get real-world data and apply DIF algorithm (for example, reach out to ETS and ask for testing data)
5. Publish package (at least get to GitHub)
6. Tada!

## Plotting space

We need a figure with three different ICCs: 1) small DIF, 2) moderate DIF, and 3) large DIF. For each simulated scenario, compute and report our DIF index so the reader understand what the number refers to (e.g., area as defined by plotting space).

## Procedure and materials  

The same simulated data as in study 1 was used. The mirt package from @R-mirt was used to compute and plot the IRT statistics. As we can see on Figure 3, the blue curves were plotted using 2PL IRT parameters (a and b), while the red curves were plotted using CTT parameters (p-values and corrected item-total correlations, re-scaling and modifying them with @kulas2017approximate formulas). To quantify the degree of difference between the two curves, the Area Between Curves was computed using @R-geiger_a's package. This procedure was done for all 100 items. 


## Results

We used `r cite_r("r-references.bib")` for all our analyses.

The area between ICC's was calculated between CTT-derived and IRT-derived ICC's. The average difference for all 100 curves was 0.35[^method]. As we can see in Figure 4, most of the data is skewed towards the lower end, indicating that out of the 100 items, most of them have areas between the curves of less than 0.35. 
For Figure 5 we plotted all the 100 ICC's that use CTT parameters, and for Figure 6 we did the same but with IRT parameters instead. Curves using both methodologies are very similar in shape and form, as we can see in the two items that we point out in each figure. 

[^method]: *Note*. Did the integral of the difference between the CTT and IRT functions using the "integrate" function in the "stats" package (base R). Did a test to confirm this accurately reflects the area between curves by creating two curves, one with high discrimination and another with low discrimination, and seeing what the area between curves was using first the geiger package and then base R. Also roughly estimated by hand this diff. Base R seems to be the more accurate method.

```{r plotting, results="hide", fig.cap="Four ICCs highlighting the difference between CTT and IRT-derivated ICCs at different levels of DIF." }
data<-read.csv("simulated_data.csv", header=FALSE)
#data$v30<-abs(data$v30-1)
library(mirt)
library(latticeExtra)
library(irtplay)
ahat<-function(x){
  r<-(((2.71828)^x)-(1/(2.71828)^x))/(2.71828-(2.71828)^x)
  
  ((0.51+(0.02*pseudob)+(0.301*pseudob^2))*r)+((0.57-(0.009*pseudob)+(0.19*pseudob^2))*r)
  
}
mod<-mirt(data, 1, itemtype="2PL")
# plot(mod, type="trace")
# 
alphas<-psych::alpha(data)
citcs<-data.frame(alphas$item.stats$r.drop)
pseudoA<-data.frame(ahat(citcs))
pseudoB<-data.frame(qnorm(colMeans(data)))
IRT_parms <- coef(mod, IRTpars = TRUE, simplify = TRUE)
irt <- IRT_parms$items
df<-as.data.frame(cbind(citcs, pseudoA, pseudoB, irt))
colnames(df)<-c("CITC", "PseudoA", "PseudoB", "a", "b", "c1", "c2")
# plot(df$PseudoA, df$a)
# plot(df$b, df$PseudoB)
lm.reg<-lm(b ~PseudoB, data=df)
 
b<-0.01479-(-1.33142*pseudoB) 
dat<-data.frame(b, alphas$item.stats$r.drop)
colnames(dat)<-c("b", "corrected item totals")
par(cex.axis=1, cex.lab=1, cex.main=2, cex.sub=0.1)
###############################################################
pseudob<-dat$b[47]
ahat<-function(x){
  r<-(((2.71828)^x)-(1/(2.71828)^x))/(2.71828-(2.71828)^x)
  
  ((0.51+(0.02*pseudob)+(0.301*pseudob^2))*r)+((0.57-(0.009*pseudob)+(0.19*pseudob^2))*r)
  
}
pseudoa<-ahat(dat$`corrected item totals`[47])
c <- 0
eq <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa*(x-pseudob))))))}
p1<-plot(mod, which.items=c(47), main=FALSE, sub="Moderate DIF \n(area between curves = 0.36 )", cex.sub=0.2, theta_lim = c(-4,4))+latticeExtra::layer(panel.curve(eq, col="red"))
################################################################
pseudob2<-dat$b[1]
ahat<-function(x){
  r<-(((2.71828)^x)-(1/(2.71828)^x))/(2.71828-(2.71828)^x)
  
  ((0.51+(0.02*pseudob2)+(0.301*pseudob2^2))*r)+((0.57-(0.009*pseudob2)+(0.19*pseudob2^2))*r)
  
}
pseudoa2<-ahat(dat$`corrected item totals`[1])
c <- 0
eq2 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa2*(x-pseudob2))))))}
p2<-plot(mod, which.items=c(1),main=FALSE, sub="Small DIF \n(area between curves = 0.03)", cex.sub=0.2, theta_lim = c(-4,4))+latticeExtra::layer(panel.curve(eq2, col="red"))
#####################################################################
pseudob3<-dat$b[54]
ahat<-function(x){
  r<-(((2.71828)^x)-(1/(2.71828)^x))/(2.71828-(2.71828)^x)
  
  ((0.51+(0.02*pseudob3)+(0.301*pseudob3^2))*r)+((0.57-(0.009*pseudob3)+(0.19*pseudob3^2))*r)
  
}
pseudoa3<-ahat(dat$`corrected item totals`[54])
c <- 0
eq3 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa3*(x-pseudob3))))))}
p3<-plot(mod, which.items=c(54), main=FALSE, sub="Small DIF \n(area between curves  = 0.09)", cex.sub=0.2, theta_lim = c(-4,4))+latticeExtra::layer(panel.curve(eq3, col="red"))
###############################################################################
pseudob4<- dat$b[25]
ahat<-function(x){
  r<-(((2.71828)^x)-(1/(2.71828)^x))/(2.71828-(2.71828)^x)
  
  ((0.51+(0.02*pseudob4)+(0.301*pseudob4^2))*r)+((0.57-(0.009*pseudob4)+(0.19*pseudob4^2))*r)
  
}
pseudoa4<-ahat(dat$`corrected item totals`[25])
c <- 0
eq4 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa4*(x-pseudob4))))))}
p4<-plot(mod, which.items=c(25), main=FALSE, sub="Large DIF \n(area between curves = 0.81)", cex.main=5, theta_lim = c(-4,4))+latticeExtra::layer(panel.curve(eq4, col="red", cex.sub=1))
###############################################################################
require(gridExtra)
grid.arrange(p2,p1,p3,p4, nrow=2, ncol=2)
##############################################################################
```

```{r histrogram, results="hide", fig.cap="Histogram of all areas between ICCs plotted using IRT parameters vs ICCs plotted using CTT parameters."}
#Area between curves
#Preparing data
library(geiger)
citcs<-data.frame(alphas$item.stats$r.drop)
pseudoA<-data.frame(ahat(citcs))
pseudoB<-b
IRT_parms <- coef(mod, IRTpars = TRUE, simplify = TRUE)
irt <- IRT_parms$items
df<-as.data.frame(cbind(citcs, pseudoA, pseudoB, irt))
colnames(df)<-c("CITC", "PseudoA", "PseudoB", "a", "b", "c1", "c2")

#calculating AUC
theta <- matrix(seq(-6,6, by=.1))
# eq4 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[25]*(x-df$PseudoB[25]))))))}
# cttB<-eq4(seq(-6,6, by=.1))
# eq4_irt<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[25]*(x-df$b[25]))))))}
# irtB<-eq4_irt(seq(-6,6, by=.1))
# geiger:::.area.between.curves(theta, cttB, irtB)
# x is the vector of x-axis values
# f1 the y-axis values for the first line
# f2 the y-axis values for the second line

#Looping
auc<-rep(NA, nrow(df))

for (i in 1:nrow(df)){
  eq_CTT<- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[i]*(x-df$PseudoB[i]))))))}
  cttB<-eq_CTT(seq(-6,6, by=.1))
  eq_IRT<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[i]*(x-df$b[i]))))))}
  irtB<-eq_IRT(seq(-6,6, by=.1))
  f1 <- approxfun(theta, cttB-irtB)     # piecewise linear function
  f2 <- function(x) abs(f1(x))                 # take the positive value
  auc[i]<-integrate(f2, -6,6)
}
auc<-unlist(auc)
h<-hist(auc, col="red", xlab="Areas Between Curves")
h
xfit<-seq(min(auc),max(auc),length=40)
yfit<-dnorm(xfit,mean=mean(auc),sd=sd(auc))
yfit <- yfit*diff(h$mids[1:2])*length(auc)
lines(xfit, yfit, col="blue", lwd=2)

```


```{r cttcurves, results="hide",fig.cap="ICCs derived from only CTT parameters (with two noteworthy ICCs annotated)."}
p_ctt<-0
p_irt<-0
colors<-rep(c("Red", "blue","yellow","orange","purple","brown","green","pink","black", "white"), 10)
eq_CTT<- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[1]*(x-df$PseudoB[1]))))))}
p_ctt<-curve(eq_CTT, xlim=c(-4,4), xlab="Level of Trait", ylab="p(1.0)")
for (i in 1:nrow(df)){
  eq_CTT<- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[i]*(x-df$PseudoB[i]))))))}
  p_ctt[i]<-curve(eq_CTT, col=colors[i], xlim=c(-4,4), add=TRUE)
  
  cttB<-eq_CTT(seq(-6,6, by=.1))
  eq_IRT<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[i]*(x-df$b[i]))))))}
  irtB<-eq_IRT(seq(-6,6, by=.1))
  auc[i]<-abs(geiger:::.area.between.curves(theta, cttB, irtB, xrange=c(-6,6)))
}
p_ctt
arrows(3.2,0.85,2.2,0.85,col="purple")
text(3.7,0.85, "Item 4",col="purple")
arrows(2,0.4,1.5,0.4,col="black")
text(2.7,0.4, "Item 24",col="black")

```


```{r irtcurves, results="hide",fig.cap="Typical ICCs derived from IRT parameters (same noteworthy items annotated)."}
eq_IRT<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[1]*(x-df$b[1]))))))}
p_irt<-curve(eq_IRT, xlim=c(-4,4), xlab="Level of Trait", ylab="p(1.0)")

for (i in 1:nrow(df)){
    eq_IRT<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[i]*(x-df$b[i]))))))}
    p_irt[i]<-curve(eq_IRT, col=colors[i], xlim=c(-4,4), add=TRUE, xlab="Level of Trait", ylab="p(x)")
}

p_irt
arrows(3,0.8,2,0.8,col="purple")
text(3.7,0.8, "Item 4",col="purple")
arrows(2,0.4,1.5,0.4,col="black")
text(2.7,0.4, "Item 24",col="black")

```



# Discussion

Important psychometric information can be gathered from ICC's, which are visual indicators typically of difficulty and discrimination. Psychometricians and other assessment specialists usually examine ICC's under the lenses of IRT and Rasch models. From a CTT orientation, item difficulty is most commonly represented by the percent of individuals answering the item correctly (also referred to as a p-value). Item discrimination can be conveyed via a few CTT indices, but the most commonly calculated and consulted index is the corrected item-total correlation. Assessment specialists who consult these CTT parameters don’t typically attempt to represent them visually, as is common in IRT and Rasch applications. However, there is perhaps little reason for them not to do so, as ICC's based on CTT parameters could provide snapshot psychometric information as valuable as those gained from IRT- or Rasch-derived ICC's. Here we first propose an application of ICC's with CTT indices, then we simulated data and quantified similarities and discrepancies between the IRT- and CTT-generated ICC's. Our hypothesis was that the Area Between Curves of these different ICC's would be small. Area between curves for 100 items was 0.35 on average. This result indicates that curves plotted with either IRT or CTT parameters show little difference. The nature of both models is mostly overlapping when it comes to plotting visual representations such as ICC's. Practitioners and researchers that don’t use IRT or Rasch models and instead opt to follow a CTT philosophy would benefit from having ICC's that use CTT statistics.

Of course there is always an intractability between the CTT item-difficulty index and respondent sample ability. The findings of previous comparison studies, however, point to the CTT estimates exhibiting some degree of invariance across respondent samples. 

If this general idea is well-received (SIOP members would seem to represent a great barometer!) we would like to stress the CTT ICC's via further and more extensive conditions. That is, are there patterns that help explain CTT ICCs that diverge from their IRT counterparts? Although our simulations did generate a range of item difficulties and discriminations, we have not yet fully explored systematic patterns of extremely difficult/easy items as well as very poorly discriminating items. If patterns emerge, we would like to model predicted discrepancies via incorporating error bars within our visualizations. 

represent some promise regarding plotted ICC's using IRT and CTT parameters. Our hypothesis was that the Area Between Curves of these different ICCs would be small. Area between curves for 100 items was 0.35 on average. This result indicates that curves plotted with either IRT or CTT parameters show little difference. The nature of both models is overlapping when it comes to plotting visual representations such as ICC's. Practitioners and researchers that don't use IRT or Rasch models and instead opt to follow a CTT philosophy would benefit from having ICC's that use CTT statistics.

Additionally, if there is interest in this general idea we would likely publish our function as a small `R` package, perhaps to supplement the `psych` package's "alpha" function, which produces corrected item-total correlations as well as p-values within the same output table (e.g., the "input" data is already available in tabular format).

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

# (APPENDIX) Appendices {-}

# Cut stuff

There have also been suggestions that the invariance property be conceptualized as a graded continuum instead of a categorical (invariant or non-invariant) population property [@hambleton1991fundamentals;@rupp2004note]. Estimates of IRT parameters across different calibration runs can be looked at for evidence of a possible lack of invariance. This doesn't happen with CTT item parameters, since they will always be sample-dependent. This dependency, however, is greatly influenced by the sampling strategy. Large scale data, truly random sampling, and large range items could give comparable CTT item and person statistics across testing populations and occasions [@kulas2017approximate]. Additionally, there are several empirical investigations that note high levels of "invariance" of CTT estimates, in some cases surpassing IRT item estimates in their capacity to have cross-sample stability [@macdonald2002monte; @fan1998item]. 

An adjustment to @lord2012applications's formula giving the functional relationship between the "non-invariant" CTT and "invariant" IRT statistics becomes useful in comparing the two methodologies, despite the supposed lack of invariance from CTT. So even though here we acknowledge that invariance is a categorical IRT property, we still follow the functional modification proposed by @kulas2017approximate, noting that having a large sample that is truly random and whose items are normally distributed and have a center at the moderate difficulty can help reduce threats to CTT "invariance". 

##NOTES
##Bias might suggest that rescaled a parameters are systematically larger than z under certain simulations (or not) Variance estimates might suggest that the standard error of rescaled values is larger than those values estimated directly (or not). If differences do exist, one could then go on to articulate the conditions under which they exist (i.e., high difficulty, low difficulty, non-normal distributions of the underlying trait), etc....




