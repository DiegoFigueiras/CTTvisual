---
title             : "Item Characteristic Curve estimation via Classical Test Theory specification"
shorttitle        : "CTT ICCs"
author: 
  - name          : "Diego Figueiras"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Dickson Hall 226"
    email         : "figueirasd1@montclair.edu"
  - name          : "John T. Kulas"
    affiliation   : "2"

affiliation:
  - id            : "1"
    institution   : "Montclair State University"
  - id            : "2"
    institution   : "eRg"

authornote: | 
  Materials for this research were provided by Educational Testing Service (ETS) and the TOEFL program. ETS does not discount or endorse the methodology, results, implications, or opinions presented by the researcher(s).

abstract: |
  Item characteristic curves (ICC's) are graphical representations of important attributes of assessment items - most commonly *difficulty* and *discrimination*. Assessment specialists who examine ICC's usually do so from within the psychometric framework of either Item Response Theory (IRT) or Rasch modeling. We propose an extension of this tradition of item characteristic visualization within the more commonly leveraged Classical Test Theory (CTT) framework. We first simulate binary (e.g., true *test*) data with varying item difficulty characteristics to derive linking coefficients between the IRT and CTT difficulty and discrimination indices. The results of these simulations provided some degree of confidence regarding functional linking coefficient invariance. Next, we simulated a sample test dataset and generated ICCs derived from both IRT and CTT frameworks. Differential item functioning (DIF) was estimated by calculating the geometric area between the IRT- and CTT-derived ogives. The average DIF estimate was low within this simulated dataset ($\overline{DIF}$ = .08 on our 13x1 dimensional plotting space). Applying the CTT-derived ICCs to six different applied tests of 20,000 real-life examinees resulted in a comparable mean DIF estimate of .12. Collectively, these results should provide some confidence to test specialists interested in creating visual representations of CTT-derived item characteristics.  An `R` package, `ctticc`, performs the ICC calculations presented in the current paper and generates ICC plots directly from CTT indices.
  
keywords          : "Classical Test Theory, Item Response Theory, item difficulty, item discrimination"
wordcount         : "X"

bibliography      : ["r-references.bib", "articles.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

csl               : "apa7.csl"
documentclass     : "apa6"
classoption       : "jou"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library(psych)
library(reticulate)

```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed, echo=FALSE, warning=FALSE, message=FALSE)
```

```{r example, include=TRUE, fig.cap="Item characteristic curves demonstrating differences in item difficulty and discrimination.", echo=FALSE, warning=FALSE, message=FALSE, out.width = "100%", out.height="80%"}


data<-read.csv("simulated_dataWINGEN.csv", header=FALSE)
#data$v30<-abs(data$v30-1)
library(mirt)
library(latticeExtra)
pseudob<-abs(qnorm(.5))

ahat<-function(x){
  r<-(((2.71828)^x)-(2.71828)^-x)/(2.71828-(2.71828)^x)
  ((0.51+(0.02*pseudob)+(0.301*pseudob^2))*r)
  
}
pseudoa<-ahat(.3)
c <- 0
#change pseudob in this line for a scale that allows negative numbers

pseudob  <-       qnorm(.02)   ## note these "p-values" operate in reverse
pseudoa  <-        ahat(.3)

pseudob2  <-  abs(qnorm(.5))
pseudoa2  <-       ahat(.7)

pseudob3  <-      qnorm(.99)
pseudoa3  <-       ahat(.7)

pseudob4  <-  abs(qnorm(.5))
pseudoa4  <-       ahat(.1)

colors<-c("Moderate Discrimination & Low Difficulty"="#aeb0af", 
          "High Discrimination & Moderate Difficulty"="#7c807d", 
          "High Discrimination & High Difficulty"="#1c1c1c", 
          "Low Discrimination & Moderate Difficulty"="#333333")

linetype<-c("Moderate Discrimination & Low Difficulty"="solid", 
          "High Discrimination & Moderate Difficulty"="dashed", 
          "High Discrimination & High Difficulty"="longdash", 
          "Low Discrimination & Moderate Difficulty"="dotted")

p <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa*(x-pseudob))))))}
p2 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa2*(x-pseudob2))))))}
p3 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa3*(x-pseudob3))))))}
p4<- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa4*(x-pseudob4))))))}

library(ggplot2)
library(scales)
base <-
  ggplot()


base+
  xlim(-3,3)+
  geom_function(fun=p, size=1.5, linetype="dotted", aes(color="Moderate Discrimination & Low Difficulty"))+
  geom_function(fun=p2, size=2, linetype="solid", aes(color="High Discrimination & Moderate Difficulty"))+
  geom_function(fun=p3, size=2, linetype="twodash", aes(color="High Discrimination & High Difficulty"))+
  geom_function(fun=p4, size=1.5, linetype="dotdash", aes(color="Low Discrimination & Moderate Difficulty"))+
  labs(x="Theta",
       y="p(1.0)",
       color="Legend")+
  scale_color_manual(values=colors)+
#  scale_linetype_manual(values = linetype)+         # didn't work (10/13/22)
  theme(legend.position="bottom")+
  guides(colour = guide_legend(nrow = 2))

```

Item characteristic curves are frequently consulted by psychometricians as visual indicators of important attributes of assessment items - most commonly *difficulty* and *discrimination*. Within these visual presentations the x-axis ranges along "trait" levels (by convention denoted with the greek $\theta$), whereas the y-axis displays probabilities of responding to the item within a given response category. In the context of true tests, the response categories are binary[^1], and the y-axis probability reflects the likelihood of a "correct" response[^2]. Assessment specialists who consult ICC's usually do so from within the psychometric framework of either Item Response Theory (IRT) or Rasch modeling. These approaches estimate the item characteristics that define the visual functions. Rasch models only estimate difficulty, and assume that differences in discrimination represent flaws in measurement [e.g., @wright1977solving]. The IRT 2 parameter logistic (2PL) and higher order models, however, estimate item discrimination in addition to item difficulty.

[^1]: With exception [see, for example, @masters1982rasch; @muraki1997generalized].

[^2]: Because the historical convention in test response is to code a correct response as "1" and an incorrect response as "0", the y-axis here is commonly denoted as "*p*(1)" or "*p*(1.0)".

When interpreting an ICC representing a true test item, the observer extracts the relationship between a respondent's trait level and the corresponding expectation of answering the item correctly. If the function transitions from low to high likelihood at a location toward the lower end of the trait (e.g., "left" on the plotting surface), this indicates that it is *relatively easy* to answer the item correctly. Stated in the parlance of IRT or Rasch traditions, it does not take much $\theta$ to have a high likelihood of answering correctly. On the contrary, if the growth in the curve occurs primarily at higher trait levels, this indicates that the item is relatively more difficult. Through the lens of IRT, if discrimination is modeled and the curve is sharp (e.g., strongly vertical), this indicates greater item discrimination across trait levels; if it is flatter, that is an indication of poorer discrimination (see Figure \@ref(fig:example) for some exemplar ICCs).

Item difficulty (the IRT *b*-parameter) is typically expressed as the trait level associated with a 50% likelihood of correct response (e.g., it is scaled to $\theta$). Item discrimination (the *a*-parameter) reflects the degree to which an item differentiates across individuals who are located relatively lower or higher on the trait and is scaled to the slope of the ICC function at the same 50% likelihood of correct response location[^3]. From a classical test theory (CTT) orientation, item difficulty is most commonly represented by the percent of individuals answering the item correctly (also referred to as a "*p* value")[^history]. Item discrimination can be conveyed via a few different CTT indices, but the most commonly calculated and consulted contemporary index is the corrected item-total correlation.

[^3]: Within the 2PL. If additional item characteristics are modeled, the *a*-parameter may be estimated at a different function location.

[^history]: Without being provided additional context, the psychometric "*p* value" is only distinguishable from the inferential statistic "*p*-value" via conventional notation adherence: omission of the grammatical dash in the case of the difficulty index. 

Assessment specialists who calculate these CTT item indices do not, by tradition, additionally represent them visually, as is common in IRT and Rasch applications. However, ICC's based on CTT indices should provide snapshot psychometric information comparably as valuable as those conveyed by IRT- or Rasch-derived item parameters. The largest obstacle to psychometricians deeming CTT-derived visuals to be of value is likely tied to the concept of invariance, which refers to IRT parameter independence across item and person estimates. However, this property is often overstated, as invariance is only attained with perfect model-data fit (which is never attained), and is also only true after being subjected to linear transformation - commonly across samples [@rupp2006understanding].[^invariance] Additionally, several comparative investigations have noted commonality between IRT and CTT difficulty and discrimination estimates as well as absolute stability of CTT estimates [e.g., @fan1998item; @kulas2017approximate; @lawson1991one].

[^invariance]: There have also been suggestions that the invariance property be conceptualized as a graded continuum instead of a categorical (invariant or non-invariant) population property [@hambleton1991fundamentals; @rupp2004note]. If this lens is adopted, the concept of "acceptable" levels of invariance further augments the rationale behind the construction of CTT-derived ICCs. 

## CTT and IRT Comparability Investigations

@fan1998item examined associations between CTT item statistics and the parameters derived from the three most popular IRT models (the 1-, 2-, and 3-parameter logistic). Correlations were very high for difficulty estimates - generally between .80 and .90. These findings converged with  both earlier and later investigations that also found strong correspondence between difficulty estimates [e.g., @lawson1991one; @macdonald2002monte]. As for item discrimination, correlations were *moderate* to high, with only a few being very low - these estimates tended to be even poorer in @macdonald2002monte's Monte Carlo investigation[^4].  

@fan1998item also investigated index invariance for all models. In theory, the primary advantage of IRT- over CTT-models is that the latter has an intractable dependency between the item and person statistics, whereas under ideal circumstances IRT parameters have no such dependency. Within  CTT examinations, for example, the average item difficulty is necessarily equivalent to the average person score - these CTT indices are merely reflective of averages computed across rows or columns. What @fan1998item reported in his study, however, did not support the purported invariant advantage of IRT parameters over CTT indices. Both CTT-derived item difficulty and discrimination indices exhibited similar levels of invariance to the IRT-derived parameters. @fan1998item in fact summarizes that the IRT and CTT frameworks "...produce very similar item and person statistics" (p.379). @macdonald2002monte's Monte Carlo simulations agreed with the @fan1998item conclusion regarding *difficulty* and person estimates, but did note superior performance of IRT relative to CTT discrimination estimates. 

[^4]: As is presented below, the relationship between the IRT and CTT discrimination indices is non-linear. The Pearson's product moment correlation was consulted in both @fan1998item and @macdonald2002monte although it is not the most appropriate index to capture the magnitude of the relationship.

## Relationship(s) between IRT and CTT Indices

In addition to the comparability studies, there have been some investigations attempting to model direct associations between IRT and CTT indices. @lord1980applications first provided a conceptual function to approximate the nonlinear relationship between the IRT *a*-parameter and the CTT discrimination index[^5]:

[^5]: @lord1980applications's CTT discrimination index is the item-test biserial correlation as opposed to the contemporarily more popular *corrected* item-total *point-biserial* correlation. 

```{=tex}
\begin{equation}
a_i\cong \frac{r_i}{\sqrt{1-r_i^2}}
\end{equation}
```
This formula was not intended for practical applications but was rather presented as an attempt to help assessment specialists who were more familiar with CTT procedures to better understand the IRT discrimination parameter. In an effort to move from the conceptual to a more practical application, @kulas2017approximate proposed a modification focused on minimizing predicted residual values (the predicted $a_i$).

The @kulas2017approximate investigations identified systematically predictive differences in the relationship between $a_i$ and $r_i$ across items with differing item difficulty values, so their alteration to @lord1980applications's formula included a moderating effect for item difficulty, with $r_i$ also being operationalized as the *point-biserial* correlation between an item's binary response and the *corrected* total test score:

```{=tex}
\begin{equation}
\hat{a_i}\cong[(.51 + .02z_g + .3z_g^2)r]+[(.57 - .009z_g + .19z_g^2)\frac{e^r-e^{-r}}{e-e^r}]
\end{equation}
```

```{r acorrected, fig.cap="@kulas2017approximate's proposed functional relationship between the IRT *a* parameter and the CTT corrected-item total correlation as a function of item difficulty (*p* value; solid = .5, dashed = .3/.7, dotted = .1/.9).", eval=TRUE}
g<-abs(qnorm(.5))
g<-0
r2<-.5          ## changed from .3 - 10/13/22
ahat<-function(r2){
  r<-(((2.71828)^r2)-(2.71828)^-r2)/(2.71828-(2.71828)^r2)
  ((0.51+(0.02*g)+(0.301*g^2))*r)
 
}

curve(ahat, from=0, to=1, ylim=c(0, 8), xname="Corrected Item-Total Correlation", ylab="IRT a-parameter")
g<-abs(qnorm(.7))
r2<-.7
curve(ahat, lty="longdash",add=TRUE )
g<-abs(qnorm(.1))
r2<-.1
curve(ahat, lty="dotted", add=TRUE)

```

Within formula (2), $g$ represents the absolute deviation from 50% responding to an item correctly and 50% responding incorrectly (e.g., a "*p* value" of .5). $z_g$ is the standard normal deviate associated with $g$. This transformation of the common *p* value was recommended by @kulas2017approximate in order to scale the CTT index along a (closer to) interval-level metric more directly analogous to the IRT *b*-parameter. Figure \@ref(fig:acorrected) presents a visual representation of the exponential relationship between the two models' discrimination indices. 
We retained the @kulas2017approximate $\hat{a_i}$ and $z_g$ indices as "starting points" in the current investigation.  

## Summary and Overall Purpose

The primary goal of the current project was to generate CTT-derived ICCs. As a standard of comparison, however, we also endeavored to evaluate the CTT-derived ICCs against their IRT-derived counterparts. These comparisons are only feasible if the CTT indices can be reasonably expressed on the IRT parameter metric (or vice versa). @fan1998item demonstrated strong associations between the CTT *p* value and IRT *b*-parameter, but did not attempt a scaling linkage. Similarly, @kulas2017approximate focused on nonlinear functional specification rather than metric of expression.  Study 1 is therefore focused on the development of linking equations such that the CTT *p* value and corrected item-total correlation may be approximated along the IRT *b*- and *a*-parameter metrics. 

```{r simulatedgraphs, results="hide", fig.cap="Shape of prescribed distributions of *p* values across Study 1 conditions."}

dparabola <- function(x){ifelse(x < 1 | x > 5, 0, (3/16)*(x-3)^2)}
library(ggplot2)
g1<-ggplot()+xlim(1,5)+geom_function(fun=dparabola, size=1.5)+theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank(),
      plot.title = element_text(size=10))+
  xlab("Condition 3")+
  ylab("density")+
  ggtitle("Inverted Normal")
g2<-ggplot()+xlim(-3,3)+geom_function(fun=dnorm, size=1.5)+theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank(),
      plot.title = element_text(size=10))+
  xlab("Condition 2")+
  ylab("density")+
  ggtitle("Normal")
g3<-ggplot()+xlim(0,1)+geom_density(aes(rbeta(1000000, 5, 2)), size=1.5)+theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank(),
      plot.title = element_text(size=10))+
  xlab("Condition 4")+
  ylab("density")+
  ggtitle("Negatively Skewed")
g4<-ggplot()+xlim(0,1)+geom_density(aes(rbeta(1000000, 2, 5)),size=1.5)+theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank(),
      plot.title = element_text(size=10))+
  xlab("Condition 5")+
  ylab("density")+
  ggtitle("Positively Skewed")
g5<-ggplot()+xlim(-0.1,1.1)+geom_function(fun=dunif, size=1.5)+theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank(), 
      plot.title = element_text(size=10))+
  xlab("Condition 1")+
  ylab("density")+
  ggtitle("Uniform")

require(gridExtra)
grid.arrange(g5,g2,g1,g3,g4, nrow=2, ncol=3)

```

# Study 1
## Procedure and methods

Study 1 focused on simulated datasets of binary item responses. The simulated data prescriptively differed in distributions of item difficulty while keeping the numbers of items (*k*=100) and "respondents" (*n*=10,000) equivalent. The first distributional form was uniform, with *p* values ranging from low (approaching 0) to high (approaching 1) at roughly equal levels of frequency. The second distribution was effectively normal with *p* values centered around 0.5. The third set of distributions was inverted normal and also centered around 0.5. The fourth distributional form was negatively skewed, and the fifth was positively skewed. Figure \@ref(fig:simulatedgraphs) provides a visual representation of idealized distributional forms that were prescribed across our simulations

```{r voldy, eval=TRUE, cache=TRUE}
library(tidyverse)
all_sims2<-read.csv("all_sims2.csv")
all_sims3<-read.csv("all_sims3.csv")
all_sims4<-read.csv("all_sims4.csv")
all_sims5<-read.csv("all_sims5.csv") #converged 8/8/2023
all_sims6<-read.csv("all_sims6.csv") #converged 8/8/2023

all_sims2$condition<-'Simulation 2'
all_sims3$condition<-'Simulation 3'
all_sims4$condition<-'Simulation 4'
all_sims5$condition<-'Simulation 5'
all_sims6$condition<-'Simulation 6'
all_sims<-rbind(all_sims2, all_sims3, all_sims4, all_sims5, all_sims6)
all_sims<-all_sims%>%filter(b<3)%>%filter(b>-3)

reg1<-lm(b~pseudob, data=all_sims)
reg2<-lm(b~pseudob+condition, data=all_sims)
reg3<-lm(b~pseudob+condition+pseudob*condition, data=all_sims)
reg1sum<-summary(reg1)
reg2sum<-summary(reg2)
reg3sum<-summary(reg3)
r2change<-anova(reg1, reg3)

# ggplot(all_sims, aes(pseudob, b, color = factor(condition))) +
#   geom_point() +
#   geom_smooth(method = "lm", se = FALSE) 
# 
# sds<-coeficients%>%group_by(condition)%>%summarise(sd_intercept=sd(intercept),
#                                                     means_intercept=mean(intercept),
#                                                     sd_slope=sd(slope),
#                                                     means_slope=mean(slope))
# # reg2<-lm(b~pseudob, data=all_sims)
# summary(reg2)
slope<-reg1$coefficients[[2]]
intercept<-reg1$coefficients[[1]]
ttest_a<-t.test(all_sims$a, all_sims$PseudoA, paired=TRUE)
ttest_b<-t.test(all_sims$b, all_sims$pseudob, paired=TRUE)
all_sims$differenceA<-all_sims$a-all_sims$PseudoA
all_sims$differenceB<-all_sims$b-all_sims$pseudob

```

For each simulation, we estimated CTT *p* values and corrected item-total correlations via the `psych` package [@R-psych]. The 2PL was also applied via the `mirt` package [@R-mirt], and *a* and *b* parameters were extracted. Regressions were applied within each simulation to predict the IRT *b* parameter from the *p* value derived $z_g$ statistic. The simulated data was derived by first specifying distributions of *p* values (via the `runif()` base-R function for the uniform and inverted distribution conditions, and `rsnorm()` for the normal and skewed distribution conditionsl). *p* values within each simulation were then referenced to generate binary item responses via application of the @R-mirt `simIrt()` function. 

Across all simulations, for items that realized extreme empirical *p* values (less than 0.02 or greater than 0.98), 200 responses were modified. For items with *p* values less than 0.02, 200 random responses of "1" were substituted. For items with *p* values greater than 0.98, 200 random responses values of "0" were imputed. This was done so the IRT models would be less likely converge on disproportionately extreme parameter estimates.

Across both Study 1 and 2, all analyses and manuscript development was accomplished via `r cite_r("r-references.bib")`.

```{r sendtoETS, eval=FALSE}
### Cross-validating with wingen simulation 
# 12/8/2022 our p-value to b-parameter simulations converged
# our CTTICC formula is now ready to share with others
# This chunk presents the information necessary to estimate CTT-ICCs and also evaluate
#their similarity to IRT generated ICCs

data<-read.csv("simulated_dataWINGEN.csv", header=FALSE) #reading wingen simulation

pseudob<-qnorm(colMeans(data))#calculating our Zg

c=0 #since we're using the 3PL specification, we set c to 0

library(mirt)
library(psych)
library(latticeExtra)

ahat<-function(x){
  r<-(((2.71828)^x)-(1/(2.71828)^x))/(2.71828-(2.71828)^x)
  
  ((0.51+(0.02*pseudob)+(0.301*pseudob^2))*r)+((0.57-(0.009*pseudob)+(0.19*pseudob^2))*r)
  
}#Formula taken from Kulas' 2017

mod<-mirt(data, 1, itemtype="2PL") #estimating the IRT-parameter from the wingen simulated data

alphas<-psych::alpha(data)#using the psych package to run alpha

citcs<-data.frame(alphas$item.stats$r.drop)#getting the corrected-item total correlations

pseudoA<-data.frame(ahat(citcs))#Applying Kula's 2017 formula to our corrected-item totals

## Getting all the parameters into one dataframe
IRT_parms <- coef(mod, IRTpars = TRUE, simplify = TRUE)#retrieving the IRT parameters from the mod object

irt <- IRT_parms$items

df<-as.data.frame(cbind(citcs, pseudoA, pseudob, irt))

colnames(df)<-c("CITC", "PseudoA", "PseudoB", "a", "b", "c1", "c2")

df$PseudoB<-0.000006957584+(-1.52731*df$PseudoB)#Using the regression coefficients computed on the simulations that converged on December 8 to modify our PseudoB

## Lines 352-363 create curves using our parameters and calculate the area between curves plotted with CTT and IRT parameters
theta <- matrix(seq(-6,6, by=.1))
auc<-rep(NA, nrow(df))

for (i in 1:nrow(df)){
  eq_CTT<- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[i]*(x-df$PseudoB[i]))))))}
  cttB<-eq_CTT(seq(-6,6, by=.1))
  eq_IRT<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[i]*(x-df$b[i]))))))}
  irtB<-eq_IRT(seq(-6,6, by=.1))
  f1 <- approxfun(theta, cttB-irtB)
  f2 <- function(x) abs(f1(x))          
  auc[i]<-integrate(f2, -6,6)
}
auc<-unlist(auc)
hist(auc)

## Plotting item 35, which has the worst DIF
pseudob<-df$b[35]
ahat<-function(x){
  r<-(((2.71828)^x)-(1/(2.71828)^x))/(2.71828-(2.71828)^x)
  
  ((0.51+(0.02*pseudob)+(0.301*pseudob^2))*r)+((0.57-(0.009*pseudob)+(0.19*pseudob^2))*r)
  
}

eq <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[35]*(x-df$PseudoB[35]))))))}
p1<-plot(mod, which.items=c(35), main=FALSE, sub="Moderate DIF \n(area between curves = 0.36 )", cex.sub=0.2, theta_lim = c(-4,4))+latticeExtra::layer(panel.curve(eq, col="red"))
p1


```

## Results

For all reported analyses, items that evidenced *b* values more extreme than |3| were excluded. We made this procedural decision because our primary interest was in placing the CTT estimates on scales most likely approximating the IRT metric. Extreme cases would have the potential to skew linking coefficients. 

Figure \@ref(fig:stackedplot) shows the distribution of five million regression slopes and intercepts estimated across our simulations. A regression predicting *b* from $z_g$ returned a $R^2$ value of `r reg1sum$r.squared`. Regarding possible differences across simulation conditions, a moderated hierarchical regression did yield a significant interaction effect ($F_{(1, 4,943,349)}$ = `r round(r2change$F[2], 5)`, *p* < .001), but this was due to our large sample size ($\Delta{R^2}$ = `r round(reg3sum$r.squared-reg1sum$r.squared,5)`). 

We first noted across conditions that the @kulas2017approximate $\hat{a_i}$ was systematically underpredicting the IRT *a*-parameter. Regressions to further modify the $\hat{a_i}$ scaling resulted in the specification of a slope coefficient modifying value of 1.72, which was retained in all subsequent analyses. We retained the slope and intercept of this regression equation to inform our $z_g$ scaling coefficients, which were *b* = `r slope` and *a* = `r intercept`. Application of these values to the $z_g$ specification theoretically places the CTT estimate on a metric more closely approximating the IRT parameter. Study 2 tests this premise empirically.

Across all five conditions, simulated distributions resulted in an average empirical *a*-estimate of `r mean(all_sims$a)` (sd = `r sd(all_sims$a)`) and average empirical *b*-estimate of `r mean(all_sims$b)` (sd = `r sd(all_sims$b)` ). The average $z_g$ was `r mean(all_sims$pseudob)` (sd = `r sd(all_sims$pseudob)`)[^bbut], and the average $\hat{a_i}$ was `r mean(all_sims$PseudoA)` (sd = `r sd(all_sims$PseudoA)`). A paired samples t-test revealed a consistent *under*prediction effect for $\hat{a_i}$ relative to the IRT *a*-parameter ($\bar{D}$ = `r mean(all_sims$differenceA)`; $t_{(4,940,181)}$ = `r ttest_a$statistic`, *p* < 0.001). Visual inspection of this effect confirmed that the underestimation became more likely with strongly discriminating items, and the effect was universal (e.g., systematic - there is likely further refinement that could be applied to the $\hat{a_i}$ formula in future applications). The average difference between the IRT *b*-parameter and $z_g$ was non-significant ($\bar{D}$ = `r mean(all_sims$differenceB)`; $t_{(4,940,181)}$ = `r ttest_b$statistic`, *p* = 0.18). 

[^bbut]: $z_g$ specification was simplified to the inverse of the standard normal deviate in all current paper analyses.  


# Study 2 

## Procedure and Methods

The purpose of Study 2 was to evaluate the comparability of IRT- and CTT-derived ICCs. We generated ICCs from one simulated and six real-world test datasets. The real-world datasets represent responses from the Test of English as a Foreign Language institutional testing program (TOEFL ITP). The TOEFL ITP has subscales of: reading (*k*=39), listening (*k*=40), and speaking (*k*=35). There were two different test forms. Datasets representing responses to items from the two different forms defining the three subscales both included responses from 10,000 examinees, and the examinee samples for the two forms did not overlap. 

The simulated data were generated using Wingen [@han2007wingen3]. One dataset with 100 binary response items and 10,000 "respondents" was requested. We generated responses derived from an effectively normal and centrally located fictional ability distribution and items with a mean *a*-parameter value of 2 (sd = 0.8) and a mean *b*-parameter value of 0 (sd = 0.5). 

Differential item functioning (DIF) was estimated via directly calculating the area between ICCs. This number reflects a two-dimensional plotting space defined by an x-axis ranging from -6 to 6 and a y-axis ranging from 0 to 1, resulting a maximal possible 2-dimensional geometric area of 13. This estimation is sensitive to both uniform as well as non-uniform DIF.  

## Results

```{r etsanalysis, message=FALSE, warning=FALSE, include=FALSE, cache=TRUE}

source("ETS/ETS_data2.R", local = knitr::knit_global())

```

```{r plotting, results="hide", fig.cap="Four ICCs highlighting the difference between CTT and IRT-derivated ICCs at different levels of DIF." }
data<-read.csv("simulated_dataWINGEN.csv", header=FALSE)
#data$v30<-abs(data$v30-1)
library(mirt)
library(latticeExtra)
ahat<-function(x){
  r<-(((2.71828)^x)-(1/(2.71828)^x))/(2.71828-(2.71828)^x)
 
  ((0.51+(0.02*pseudob)+(0.301*pseudob^2))*r)+((0.57-(0.009*pseudob)+(0.19*pseudob^2))*r)
 
}
set.seed(123)
mod<-mirt(data, 1, itemtype="2PL")
# plot(mod, type="trace")
#
alphas<-psych::alpha(data)
citcs<-data.frame(alphas$item.stats$r.drop)
pseudoA<-data.frame(ahat(citcs))
pseudoB<-data.frame(qnorm(colMeans(data)))
IRT_parms <- coef(mod, IRTpars = TRUE, simplify = TRUE)
irt <- IRT_parms$items
df<-as.data.frame(cbind(citcs, pseudoA, pseudoB, irt))
colnames(df)<-c("CITC", "PseudoA", "PseudoB", "a", "b", "c1", "c2")
# plot(df$PseudoA, df$a)
# plot(df$b, df$PseudoB)
lm.reg<-lm(b ~PseudoB, data=df)
 
b<-0.01479-(-1.33142*pseudoB)
dat<-data.frame(b, alphas$item.stats$r.drop)
colnames(dat)<-c("b", "corrected item totals")
theta <- matrix(seq(-6,6, by=.1))
auc<-rep(NA, ncol(set0))

for (i in 1:nrow(df)){
  eq_CTT<- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[i]*(x-df$PseudoB[i]))))))}
  cttB<-eq_CTT(seq(-6,6, by=.1))
  eq_IRT<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[i]*(x-df$b[i]))))))}
  irtB<-eq_IRT(seq(-6,6, by=.1))
  f1 <- approxfun(theta, cttB-irtB)
  f2 <- function(x) abs(f1(x))          
  auc[i]<-integrate(f2, -6,6)
}
auc0<-data.frame(unlist(auc))
par(cex.axis=1, cex.lab=1, cex.main=2, cex.sub=0.1)
###############################################################
pseudob<-dat$b[82]
ahat<-function(x){
  r<-(((2.71828)^x)-(1/(2.71828)^x))/(2.71828-(2.71828)^x)
 
  ((0.51+(0.02*pseudob)+(0.301*pseudob^2))*r)+((0.57-(0.009*pseudob)+(0.19*pseudob^2))*r)
 
}
pseudoa<-ahat(dat$`corrected item totals`[82])
c <- 0
eq <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa*(x-pseudob))))))}
p1<-plot(mod, which.items=c(82), main=FALSE, sub="Moderate DIF \n(area between curves = 0.3 )", cex.sub=0.2, theta_lim = c(-4,4))+latticeExtra::layer(panel.curve(eq, col="red"))
################################################################
pseudob2<-dat$b[80]
ahat<-function(x){
  r<-(((2.71828)^x)-(1/(2.71828)^x))/(2.71828-(2.71828)^x)
 
  ((0.51+(0.02*pseudob2)+(0.301*pseudob2^2))*r)+((0.57-(0.009*pseudob2)+(0.19*pseudob2^2))*r)
 
}
pseudoa2<-ahat(dat$`corrected item totals`[80])
c <- 0
eq2 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa2*(x-pseudob2))))))}
p2<-plot(mod, which.items=c(80),main=FALSE, sub="Small DIF \n(area between curves = 0.03)", cex.sub=0.2, theta_lim = c(-4,4))+latticeExtra::layer(panel.curve(eq2, col="red"))
#####################################################################
pseudob3<-dat$b[70]
ahat<-function(x){
  r<-(((2.71828)^x)-(1/(2.71828)^x))/(2.71828-(2.71828)^x)
 
  ((0.51+(0.02*pseudob3)+(0.301*pseudob3^2))*r)+((0.57-(0.009*pseudob3)+(0.19*pseudob3^2))*r)
 
}
pseudoa3<-ahat(dat$`corrected item totals`[70])
c <- 0
eq3 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa3*(x-pseudob3))))))}
p3<-plot(mod, which.items=c(70), main=FALSE, sub="Average empirical DIF \n(area between curves  = 0.11)", cex.sub=0.2, theta_lim = c(-4,4))+latticeExtra::layer(panel.curve(eq3, col="red"))
###############################################################################
pseudob4<- dat$b[49]
ahat<-function(x){
  r<-(((2.71828)^x)-(1/(2.71828)^x))/(2.71828-(2.71828)^x)
 
  ((0.51+(0.02*pseudob4)+(0.301*pseudob4^2))*r)+((0.57-(0.009*pseudob4)+(0.19*pseudob4^2))*r)
 
}
pseudoa4<-ahat(dat$`corrected item totals`[49])
c <- 0
eq4 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(pseudoa4*(x-pseudob4))))))}
p4<-plot(mod, which.items=c(49), main=FALSE, sub="Large DIF \n(area between curves = 0.97)", cex.main=5, theta_lim = c(-4,4))+latticeExtra::layer(panel.curve(eq4, col="red", cex.sub=1))
###############################################################################
require(gridExtra)
grid.arrange(p2,p3,p1,p4, nrow=2, ncol=2)
##############################################################################
```

```{r stackedplot, results="hide", fig.cap="Individual intercepts and slopes grouped by study 2 simulation."}
coeficients<-read.csv("coefficients.csv")
coeficients$simulation[coeficients$simulation=='Simulation 2']<-'Uniform'
coeficients$simulation[coeficients$simulation=='Simulation 3']<-'Normal'
coeficients$simulation[coeficients$simulation=='Simulation 4']<-'Inverted'
coeficients$simulation[coeficients$simulation=='Simulation 5']<-'Negative S'
coeficients$simulation[coeficients$simulation=='Simulation 6']<-'Positive S'
coeficients$simulation <- factor(coeficients$simulation, levels = c('Uniform', 'Normal', 'Inverted', 'Negative S', 'Positive S'))

library(ggplot2)
library(ggthemes)
library(gridExtra)
library(viridis)

h1<-ggplot(data = coeficients, aes(x = intercept)) + geom_histogram(bins = 500) + facet_grid(simulation~.)+xlim(-.5,.5)+theme(strip.text.y = element_blank())

h2<-ggplot(data = coeficients, aes(x = slope)) + geom_histogram(bins = 500) + facet_grid(simulation~.)+xlim(-2,-1)+theme(axis.text.y = element_blank(), axis.title.y= element_blank())

gridExtra::grid.arrange(h1,h2, nrow=1, ncol=2)


```

```{r histrogram, results="hide", fig.cap="Histogram of geometric areas between ICCs plotted with IRT parameters versus those plotted with CTT statistics.", cache=TRUE}
#Area between curves
#Preparing data
library(geiger)
library(ctticc)
library(mirt)
############################### Wingen ############################################################################
data<-read.csv("simulated_dataWINGEN.csv", header=FALSE)
df<-data.frame(ctticc(data))
colnames(df)<-c("PseudoB", "PseudoA")
mod<-mirt(data, 1, itemtype="2PL")
IRT_parms <- coef(mod, IRTpars = TRUE, simplify = TRUE)
irt <- IRT_parms$items
df<-as.data.frame(cbind(df, irt))
colnames(df)<-c("PseudoB", "PseudoA", "a", "b", "c1", "c2")
#calculating AUC
theta <- matrix(seq(-6,6, by=.1))
# eq4 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[25]*(x-df$PseudoB[25]))))))}
# cttB<-eq4(seq(-6,6, by=.1))
# eq4_irt<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[25]*(x-df$b[25]))))))}
# irtB<-eq4_irt(seq(-6,6, by=.1))
# geiger:::.area.between.curves(theta, cttB, irtB)
# x is the vector of x-axis values
# f1 the y-axis values for the first line
# f2 the y-axis values for the second line

#Looping
c<-0
auc<-rep(NA, nrow(df))

for (i in 1:nrow(df)){
  eq_CTT<- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[i]*(x-df$PseudoB[i]))))))}
  cttB<-eq_CTT(seq(-6,6, by=.1))
  eq_IRT<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[i]*(x-df$b[i]))))))}
  irtB<-eq_IRT(seq(-6,6, by=.1))
  f1 <- approxfun(theta, cttB-irtB)     # piecewise linear function
  f2 <- function(x) abs(f1(x))                 # take the positive value
  auc[i]<-integrate(f2, -6,6)
}
auc<-data.frame(unlist(auc))
auc$condition<-"wingen"


################################# ETS #################################################################

############################ set 1 #################################################################3
df1<-data.frame(ctticc(set1))
colnames(df1)<-c("PseudoB", "PseudoA")
mod<-mirt(set1, 1, itemtype="2PL")
IRT_parms <- coef(mod, IRTpars = TRUE, simplify = TRUE)
irt <- IRT_parms$items
df1<-as.data.frame(cbind(df1, irt))
colnames(df1)<-c("PseudoB", "PseudoA", "a", "b", "c1", "c2")
#calculating AUC
theta <- matrix(seq(-6,6, by=.1))
# eq4 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[25]*(x-df$PseudoB[25]))))))}
# cttB<-eq4(seq(-6,6, by=.1))
# eq4_irt<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[25]*(x-df$b[25]))))))}
# irtB<-eq4_irt(seq(-6,6, by=.1))
# geiger:::.area.between.curves(theta, cttB, irtB)
# x is the vector of x-axis values
# f1 the y-axis values for the first line
# f2 the y-axis values for the second line

#Looping
auc1<-rep(NA, nrow(df1))

for (i in 1:nrow(df1)){
  eq_CTT<- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df1$PseudoA[i]*(x-df1$PseudoB[i]))))))}
  cttB<-eq_CTT(seq(-6,6, by=.1))
  eq_IRT<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df1$a[i]*(x-df1$b[i]))))))}
  irtB<-eq_IRT(seq(-6,6, by=.1))
  f1 <- approxfun(theta, cttB-irtB)     # piecewise linear function
  f2 <- function(x) abs(f1(x))                 # take the positive value
  auc1[i]<-integrate(f2, -6,6)
}

auc1<-data.frame(unlist(auc1))
auc1$condition<-"Listening 1"

######################################### set 2 #######################################################

df2<-data.frame(ctticc(set2))
colnames(df2)<-c("PseudoB", "PseudoA")
mod<-mirt(set2, 1, itemtype="2PL")
IRT_parms <- coef(mod, IRTpars = TRUE, simplify = TRUE)
irt <- IRT_parms$items
df2<-as.data.frame(cbind(df2, irt))
colnames(df2)<-c("PseudoB", "PseudoA", "a", "b", "c1", "c2")
#calculating AUC
theta <- matrix(seq(-6,6, by=.1))
# eq4 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[25]*(x-df$PseudoB[25]))))))}
# cttB<-eq4(seq(-6,6, by=.1))
# eq4_irt<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[25]*(x-df$b[25]))))))}
# irtB<-eq4_irt(seq(-6,6, by=.1))
# geiger:::.area.between.curves(theta, cttB, irtB)
# x is the vector of x-axis values
# f1 the y-axis values for the first line
# f2 the y-axis values for the second line

#Looping
auc2<-rep(NA, nrow(df2))

for (i in 1:nrow(df2)){
  eq_CTT<- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df2$PseudoA[i]*(x-df2$PseudoB[i]))))))}
  cttB<-eq_CTT(seq(-6,6, by=.1))
  eq_IRT<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df2$a[i]*(x-df2$b[i]))))))}
  irtB<-eq_IRT(seq(-6,6, by=.1))
  f1 <- approxfun(theta, cttB-irtB)     # piecewise linear function
  f2 <- function(x) abs(f1(x))                 # take the positive value
  auc2[i]<-integrate(f2, -6,6)
}
auc2<-data.frame(unlist(auc2))
auc2$condition<-"Speaking 1"

########################################### set 3 ###################################################
df3<-data.frame(ctticc(set3))
colnames(df3)<-c("PseudoB", "PseudoA")
mod<-mirt(set3, 1, itemtype="2PL")
IRT_parms <- coef(mod, IRTpars = TRUE, simplify = TRUE)
irt <- IRT_parms$items
df3<-as.data.frame(cbind(df3, irt))
colnames(df3)<-c("PseudoB", "PseudoA", "a", "b", "c1", "c2")
#calculating AUC
theta <- matrix(seq(-6,6, by=.1))
# eq4 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[25]*(x-df$PseudoB[25]))))))}
# cttB<-eq4(seq(-6,6, by=.1))
# eq4_irt<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[25]*(x-df$b[25]))))))}
# irtB<-eq4_irt(seq(-6,6, by=.1))
# geiger:::.area.between.curves(theta, cttB, irtB)
# x is the vector of x-axis values
# f1 the y-axis values for the first line
# f2 the y-axis values for the second line

#Looping
auc3<-rep(NA, nrow(df3))

for (i in 1:nrow(df3)){
  eq_CTT<- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df3$PseudoA[i]*(x-df3$PseudoB[i]))))))}
  cttB<-eq_CTT(seq(-6,6, by=.1))
  eq_IRT<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df3$a[i]*(x-df3$b[i]))))))}
  irtB<-eq_IRT(seq(-6,6, by=.1))
  f1 <- approxfun(theta, cttB-irtB)     # piecewise linear function
  f2 <- function(x) abs(f1(x))                 # take the positive value
  auc3[i]<-integrate(f2, -6,6)
}

auc3<- data.frame(unlist(auc3))
auc3$condition<-"Reading 1"
names(auc3)[1]<-"auc"

################################################# set 4 ################################################
df4<-data.frame(ctticc(set4))
colnames(df4)<-c("PseudoB", "PseudoA")
mod<-mirt(set4, 1, itemtype="2PL")
IRT_parms <- coef(mod, IRTpars = TRUE, simplify = TRUE)
irt <- IRT_parms$items
df4<-as.data.frame(cbind(df4, irt))
colnames(df4)<-c("PseudoB", "PseudoA", "a", "b", "c1", "c2")
#calculating AUC
theta <- matrix(seq(-6,6, by=.1))
# eq4 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[25]*(x-df$PseudoB[25]))))))}
# cttB<-eq4(seq(-6,6, by=.1))
# eq4_irt<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[25]*(x-df$b[25]))))))}
# irtB<-eq4_irt(seq(-6,6, by=.1))
# geiger:::.area.between.curves(theta, cttB, irtB)
# x is the vector of x-axis values
# f1 the y-axis values for the first line
# f2 the y-axis values for the second line

#Looping
auc4<-rep(NA, nrow(df4))

for (i in 1:nrow(df4)){
  eq_CTT<- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df4$PseudoA[i]*(x-df4$PseudoB[i]))))))}
  cttB<-eq_CTT(seq(-6,6, by=.1))
  eq_IRT<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df4$a[i]*(x-df4$b[i]))))))}
  irtB<-eq_IRT(seq(-6,6, by=.1))
  f1 <- approxfun(theta, cttB-irtB)     # piecewise linear function
  f2 <- function(x) abs(f1(x))                 # take the positive value
  auc4[i]<-integrate(f2, -6,6)
}

auc4<- data.frame(unlist(auc4))
auc4$condition<-"Listening 2"

##################################### set 5 ##############################################################

df5<-data.frame(ctticc(set5))
colnames(df5)<-c("PseudoB", "PseudoA")
mod<-mirt(set5, 1, itemtype="2PL")
IRT_parms <- coef(mod, IRTpars = TRUE, simplify = TRUE)
irt <- IRT_parms$items
df5<-as.data.frame(cbind(df5, irt))
colnames(df5)<-c("PseudoB", "PseudoA", "a", "b", "c1", "c2")
#calculating AUC
theta <- matrix(seq(-6,6, by=.1))
# eq4 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[25]*(x-df$PseudoB[25]))))))}
# cttB<-eq4(seq(-6,6, by=.1))
# eq4_irt<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[25]*(x-df$b[25]))))))}
# irtB<-eq4_irt(seq(-6,6, by=.1))
# geiger:::.area.between.curves(theta, cttB, irtB)
# x is the vector of x-axis values
# f1 the y-axis values for the first line
# f2 the y-axis values for the second line

#Looping
auc5<-rep(NA, nrow(df5))

for (i in 1:nrow(df5)){
  eq_CTT<- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df5$PseudoA[i]*(x-df5$PseudoB[i]))))))}
  cttB<-eq_CTT(seq(-6,6, by=.1))
  eq_IRT<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df5$a[i]*(x-df5$b[i]))))))}
  irtB<-eq_IRT(seq(-6,6, by=.1))
  f1 <- approxfun(theta, cttB-irtB)     # piecewise linear function
  f2 <- function(x) abs(f1(x))                 # take the positive value
  auc5[i]<-integrate(f2, -6,6)
}
auc5<- as.data.frame(unlist(auc5))
auc5$condition<-"Speaking 2"

######################################## set 6 ########################################################

df6<-data.frame(ctticc(set6))
colnames(df6)<-c("PseudoB", "PseudoA")
mod<-mirt(set6, 1, itemtype="2PL")
IRT_parms <- coef(mod, IRTpars = TRUE, simplify = TRUE)
irt <- IRT_parms$items
df6<-as.data.frame(cbind(df6, irt))
colnames(df6)<-c("PseudoB", "PseudoA", "a", "b", "c1", "c2")
#calculating AUC
theta <- matrix(seq(-6,6, by=.1))
# eq4 <- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$PseudoA[25]*(x-df$PseudoB[25]))))))}
# cttB<-eq4(seq(-6,6, by=.1))
# eq4_irt<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df$a[25]*(x-df$b[25]))))))}
# irtB<-eq4_irt(seq(-6,6, by=.1))
# geiger:::.area.between.curves(theta, cttB, irtB)
# x is the vector of x-axis values
# f1 the y-axis values for the first line
# f2 the y-axis values for the second line

#Looping
auc6<-rep(NA, nrow(df6))

for (i in 1:nrow(df6)){
  eq_CTT<- function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df6$PseudoA[i]*(x-df6$PseudoB[i]))))))}
  cttB<-eq_CTT(seq(-6,6, by=.1))
  eq_IRT<-function(x){c + ((1-c)*(1/(1+2.71828^(-1.7*(df6$a[i]*(x-df6$b[i]))))))}
  irtB<-eq_IRT(seq(-6,6, by=.1))
  f1 <- approxfun(theta, cttB-irtB)     # piecewise linear function
  f2 <- function(x) abs(f1(x))                 # take the positive value
  auc6[i]<-integrate(f2, -6,6)
}
auc6<- data.frame(unlist(auc6))
auc6$condition<-"Reading 2"
names(auc)[1]<-"auc"
names(auc1)[1]<-"auc"
names(auc2)[1]<-"auc"
names(auc3)[1]<-"auc"
names(auc4)[1]<-"auc"
names(auc5)[1]<-"auc"
names(auc6)[1]<-"auc"

auc_together<-rbind(auc, auc1, auc2, auc3, auc4, auc5, auc6)
ggplot(data = auc_together, aes(x = auc)) + geom_histogram(bins=30, fill="cyan", color="black") +  facet_grid(condition~.)+geom_density(color="red")+theme(axis.title.y.right = element_text(size=rel(10)))



auc_together$condition<-as.factor(auc_together$condition)
reg<-lm(auc~condition, auc_together)
auc_anova<-anova(reg)





```



```{r diff, message=FALSE, warning=FALSE, paged.print=FALSE, cache=TRUE, eval=FALSE}

source("diff_analysis.R", local = knitr::knit_global())

```

The mirt package [@R-mirt] was again retained for 2PL estimation. Across all 7 datasets, DIF ranged from a smallest value of `r min(auc_together$auc)` to a greatest value of `r max(auc_together$auc)`. Figure \@ref(fig:histrogram) presents histograms of the individual DIF magnitudes organized by focal test. The average DIF across all 7 datasets was `r mean(auc_together$auc)` (sd=`r sd(auc_together$auc)`). There was no difference in average DIF across the 7 datasets ($F_{(6, 321)}$ =`r auc_anova[1,4]`, *p* \> .05). Only 17 (5.2%) of the 328 total investigated items exhibited a DIF above 0.3. The simulated test data returned an average DIF estimate of `r mean(data0$diff)` whereas the TOEFL tests returned an average estimate of `r mean(df_plot$diff)` across `r nrow(df_plot)` investigated items.

Figure \@ref(fig:plotting) presents ICCs representing items that exhibited small, empirically average, moderate, and notably large degrees of DIF ("small", "moderate", and "large" are subjective author specifications based on the omnibus distribution of DIF across all 7 tests). Here, the blue curves were plotted using 2PL IRT parameters (a and b), while the red curves were plotted using CTT parameters [p-values and corrected item-total correlations, re-scaled with Study 1 modified @kulas2017approximate's formulas].

[^8]: Note that the theta range in figure \@ref(fig:plotting) is between -4 to 4 to be consistent with convention -- the DIF was actually calculated along values ranging from -6 to 6. 

# Discussion

Valuable psychometric information can be quickly extracted from ICC's. Assessment specialists, by tradition, exclusively seek to generate ICC's within the frameworks of IRT and/or Rasch models. Of course there will always be an intractability between the CTT item indices and respondent sample abilities. The findings of previous comparison studies, however, point to the CTT estimates exhibiting a moderate-to-high degree of: 1) comparability to IRT parameters, and 2) invariance across respondent samples. The proposal of the current presentation is that ICC's derived from CTT statistics may provide snapshot psychometric information similar in value to those derived from IRT parameters. There's little *practical* (as well as perhaps empirical) reason for assessment specialists who rely on a CTT framework to be denied the privilege of ICCs. 

The noted geometric areas between curves for all investigated items was, on average, low.

Common inferential DIF indices such as ... require the specification of parameter variance and covariance estimates, which were not sought in the case of the CTT indices. Future investigations may wish to attempt an application of inferential DIF indices to obtain estimates more comparable to others found in the published and commercial literatures. 

Future improvements could stress the CTT ICC's via further and more extensive simulations. That is, are there patterns that help isolate the CTT ICCs that diverge from the IRT-derived ICCs? Although our simulations did generate a range of item difficulties and discriminations, we have not yet fully explored systematic patterns of extremely difficult/easy items as well as very poorly discriminating items. Visual inspection of the current study indices were consistent in that items with *a*-parameters above 1 occasionally had consistently underestimated $\hat{a}$ values.$Z_g$ is also consistently overpredicting at extreme values (regardless of whether it was an extreme positive or an extreme negative, the pattern is consistent). Further refinement can be made. If patterns emerge, we would like to model predicted discrepancies via incorporating error bars within our visualizations. 

CTT item statistics will always be sample-dependent. This dependency, however, is greatly influenced by the sampling strategy. Large scale data, truly random sampling, and large range items could give comparable CTT item and person statistics across testing populations and occasions [@kulas2017approximate]. Additionally, there are several empirical investigations that note high levels of "invariance" of CTT estimates, in some cases surpassing IRT item estimates in their capacity to have cross-sample stability [@macdonald2002monte; @fan1998item].

The current specifications are available to apply via a small `R` package. Although scaled inventory responses are more common in Psychological assessment applications, We do not believe a visual representation of the polytomous item response function (IRF) would be as practically informative, and do not foresee extensions to inventory response.

represent some promise regarding plotted ICC's using IRT and CTT parameters. Our hypothesis was that the Area Between Curves of these different ICCs would be small. Area between curves for 100 items was 0.35 on average. This result indicates that curves plotted with either IRT or CTT parameters show little difference. The nature of both models is overlapping when it comes to plotting visual representations such as ICC's. Practitioners and researchers that don't use IRT or Rasch models and instead opt to follow a CTT philosophy would benefit from having ICC's that use CTT statistics.

IRT analyses are also data hungry. These CTT-derived ICC estimates may be useful to individuals who wish to ultimately apply IRT, but are limited in... [maybe not]

Because the ICCs are derived from the same individuals, the current application does not require the same parameter scaling that is typically required in DIF investigations where the ICCs reflect responses from different groups that likely have different underlying distributions of ability. 

IRT models can converge with wildly large *b*-parameter estimates with extremely difficult or easy items [PROBABLY NEED CITE OR PERSONAL EXPERIENCE STATEMENT]. This is possibly the reason previous investigations have noted an advantage to the CTT-derived difficulty estimates with regard to index invariance [e.g., @fan1998item; @lawson1991one]. The IRT estimate is bound by positive and negative infinity - the CTT estimate is finite and does not suffer this possibility.

## Tada (the end)

There is a popular @thorndike1982educational quote regarding the (at the time) rising popularity of IRT models that has been noted in previous IRT-CTT investigations [e.g., @fan1998item; @macdonald2002monte]. The quote also bears inclusion here:

>For the large bulk of testing, both with locally developed and with standardized tests, I doubt that there will be a great deal of change. The items that we will select for a test will not be much different from those we would have selected with earlier procedures, and the resulting tests will continue to have much the same properties. (p. 12)

We agree, and offer a gift to CTT practitioners from the IRT tradition - not 

\newpage

# References

```{r}
r_refs("r-references.bib", append=FALSE)         ## append=FALSE auto-updates package version 
```

```{=tex}
\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
```
::: {#refs custom-style="Bibliography"}
:::

```{=tex}
\endgroup
```
# (APPENDIX) Appendices {.unnumbered}

##NOTES ##Bias might suggest that rescaled a parameters are systematically larger than z under certain simulations (or not) Variance estimates might suggest that the standard error of rescaled values is larger than those values estimated directly (or not). If differences do exist, one could then go on to articulate the conditions under which they exist (i.e., high difficulty, low difficulty, non-normal distributions of the underlying trait), etc...

metrics Because of simulation data with consistent under-prediction, modifications were applied to both indices. A slight alteration to this index was made in the current investigation whereby the simpler direct inverse of the standard normal deviate was retained.

*Note* Find citations about extreme b parameters due to poor fit. 
